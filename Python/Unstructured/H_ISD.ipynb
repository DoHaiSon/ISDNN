{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSmcwLZuptDU"
   },
   "source": [
    "# Proposed Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG5fZn_ypunP"
   },
   "source": [
    "# **Biblioheque**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "--SvzvIspu-U"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from scipy.stats import rice\n",
    "# import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import sys\n",
    "import timeit\n",
    "import os\n",
    "\n",
    "# torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBk01Vu0pvMm"
   },
   "source": [
    "# class to save results in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "06IymVCkpvYS"
   },
   "outputs": [],
   "source": [
    "class Record:\n",
    "    def __init__(self, TextName):\n",
    "        self.out_file = open(TextName, 'a')\n",
    "        self.old_stdout = sys.stdout\n",
    "        sys.stdout = self\n",
    "\n",
    "    def write(self, text):\n",
    "        self.old_stdout.write(text)\n",
    "        self.out_file.write(text)\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout = self.old_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xPamGjNpv8E"
   },
   "source": [
    "# **slicer the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dlQdA0xHpwF7"
   },
   "outputs": [],
   "source": [
    "def slicer(data):\n",
    "    dataI = data[slice(0, len(data), 2)]\n",
    "    dataQ = data[slice(1, len(data), 2)]\n",
    "    return(dataI, dataQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOoiIo_LpwQl"
   },
   "source": [
    "# **Modulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xIftC8fvpwir"
   },
   "outputs": [],
   "source": [
    "def mapper_16QAM(QAM16, data):\n",
    "    map0 = 2*data[slice(0, len(data), 2)] + data[slice(1, len(data), 2)]\n",
    "    map0 = list(map(int, map0))\n",
    "    dataMapped = []\n",
    "    for i in range(len(map0)):\n",
    "        dataMapped.append(QAM16[map0[i]])\n",
    "    return(dataMapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AYhPgAIcqtGT"
   },
   "outputs": [],
   "source": [
    "def calculate_bits(Modulation,NumSubcarriers,NumDataSymb):\n",
    "    if Modulation=='QPSK':\n",
    "        Nbpscs=2\n",
    "    elif Modulation=='16QAM':\n",
    "        Nbpscs=4\n",
    "    return NumDataSymb*NumSubcarriers*Nbpscs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgX6tiqFpwvb"
   },
   "source": [
    "# **generate noise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2LQwZvGaprKZ"
   },
   "outputs": [],
   "source": [
    "def AWGN(IFsig, SNR):\n",
    "    dP = np.zeros(len(IFsig))\n",
    "    P = 0\n",
    "\n",
    "    for i in range(len(IFsig)):\n",
    "        dP[i] = abs(IFsig[i])**2\n",
    "        P = P + dP[i]\n",
    "\n",
    "    P = P/len(IFsig)\n",
    "    gamma = 10**(SNR/10)\n",
    "    N0 = P/gamma\n",
    "    n = ((N0/2)**(0.5))*np.random.standard_normal(len(IFsig))\n",
    "    IF_n = np.zeros((len(IFsig),1))\n",
    "\n",
    "    for i in range(len(IFsig)):\n",
    "        IF_n[i,:] = IFsig[i] + n[i]\n",
    "\n",
    "    return(IF_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate channel model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_channel(Nr, Nt, type):\n",
    "    if (type == 'gauss'):\n",
    "        return (np.random.normal(size=(Nr,Nt))+1j*np.random.normal(size=(Nr,Nt)))/np.sqrt(2)\n",
    "    if (type == 'rayleigh'):\n",
    "        return (np.random.rayleigh(scale=(1/np.sqrt(2)), size=(Nr,Nt)) + 1j*np.random.rayleigh(scale=(1/np.sqrt(2)), size=(Nr,Nt)))/np.sqrt(2)\n",
    "    if (type == 'rician'):\n",
    "        b = 1/np.sqrt(2)\n",
    "        return (rice.rvs(b, size=(Nr,Nt)) + 1j*rice.rvs(b, size=(Nr,Nt)))/np.sqrt(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eciNtnFjq2yd"
   },
   "source": [
    "# **Generate Dataset**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "AE4Q6jZCq3CY"
   },
   "outputs": [],
   "source": [
    "DataSet_x   = []  # x dataset after modulation\n",
    "DataSet_y   = []  # y dataset\n",
    "DataSet_HH  = []  # H dataset\n",
    "DataSet_b   = []  # binary dataset\n",
    "SNR_min_dB  = 0\n",
    "SNR_max_dB  = 20\n",
    "step_dB     = 5\n",
    "num_dB      = int((SNR_max_dB - SNR_min_dB) / step_dB) + 1\n",
    "\n",
    "SNR         = np.linspace(SNR_min_dB, SNR_max_dB, num=num_dB)\n",
    "\n",
    "\n",
    "Nt = 8             # Tx: 8\n",
    "Nr = 64            # Rx: 128\n",
    "N_samp = 4000\n",
    "\n",
    "\n",
    "def Gen_dataset(mode, snr, imperfect, N_samp):    \n",
    "    DataSet_x   = []  # x dataset after modulation\n",
    "    DataSet_y   = []  # y dataset\n",
    "    DataSet_H   = []  \n",
    "    DataSet_HH  = []\n",
    "\n",
    "    NumSubcarriers = 1\n",
    "    Modulation = '16QAM'\n",
    "    QAM16 = [-1, -0.333, 0.333, 1]\n",
    "    NumDataSymb = 1\n",
    "    N_type = 'gauss'\n",
    "\n",
    "    if mode == 'train':\n",
    "        for snr in SNR:\n",
    "            for runIdx in range(0, N_samp):      # ! 20000 x Nt: samples\n",
    "                H = Generate_channel(Nt, Nr, N_type)\n",
    "                HH = np.concatenate((np.concatenate((H.real, H.imag), axis=1),\n",
    "                                    np.concatenate((-H.imag, H.real), axis=1)), axis=0)\n",
    "                x = np.zeros((2*Nt, NumSubcarriers))\n",
    "                a = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
    "                DataRaw = np.zeros((Nt, a))\n",
    "                for t in range(Nt):\n",
    "                    #\"data symbol generate\"\n",
    "                    NumBits = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
    "                    bit = np.random.randint(1, 3, NumBits)-1\n",
    "                    DataRaw[t, :] = bit\n",
    "                    for j in range(4):\n",
    "                        DataSet_b.append(bit[j])\n",
    "                    I = np.zeros((1, a))\n",
    "                    I[0, :] = DataRaw[t, :]\n",
    "                    (dataI, dataQ) = slicer(I[0])\n",
    "\n",
    "                    # Mapper\n",
    "                    mapI = mapper_16QAM(QAM16, dataI)\n",
    "                    mapQ = mapper_16QAM(QAM16, dataQ)\n",
    "                    x[t] = mapI[0]\n",
    "                    x[t+Nt] = mapQ[0]\n",
    "\n",
    "                # transpose\n",
    "                x = x.transpose()\n",
    "\n",
    "                y_wo_noise = np.matmul(x, HH)\n",
    "\n",
    "                # noise\n",
    "                noise = AWGN(y_wo_noise.transpose(), snr)\n",
    "\n",
    "                y = y_wo_noise + noise.transpose()\n",
    "\n",
    "                DataSet_x.append(x)    # ! I, Q sample distance by Nt.\n",
    "                DataSet_y.append(y)                 # ! output sample\n",
    "                \n",
    "                # Imperfect channel: 5%\n",
    "                # coef = (2*np.random.randint(0,2,size=HH.shape) - 1)\n",
    "                # HH = HH + coef * HH * 0.05\n",
    "                DataSet_HH.append(HH)\n",
    "                DataSet_H.append(H)               # ! Generated channel\n",
    "                \n",
    "    else:\n",
    "        for runIdx in range(0, N_samp):      # ! 20000 x Nt: samples\n",
    "            H = Generate_channel(Nt, Nr, N_type)\n",
    "            HH = np.concatenate((np.concatenate((H.real, H.imag), axis=1),\n",
    "                                np.concatenate((-H.imag, H.real), axis=1)), axis=0)\n",
    "            x = np.zeros((2*Nt, NumSubcarriers))\n",
    "            a = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
    "            DataRaw = np.zeros((Nt, a))\n",
    "            for t in range(Nt):\n",
    "                #\"data symbol generate\"\n",
    "                NumBits = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
    "                bit = np.random.randint(1, 3, NumBits)-1\n",
    "                DataRaw[t, :] = bit\n",
    "                for j in range(4):\n",
    "                    DataSet_b.append(bit[j])\n",
    "                I = np.zeros((1, a))\n",
    "                I[0, :] = DataRaw[t, :]\n",
    "                (dataI, dataQ) = slicer(I[0])\n",
    "\n",
    "                # Mapper\n",
    "                mapI = mapper_16QAM(QAM16, dataI)\n",
    "                mapQ = mapper_16QAM(QAM16, dataQ)\n",
    "                x[t] = mapI[0]\n",
    "                x[t+Nt] = mapQ[0]\n",
    "\n",
    "            # transpose\n",
    "            x = x.transpose()\n",
    "\n",
    "            y_wo_noise = np.matmul(x, HH)\n",
    "\n",
    "            # noise\n",
    "            noise = AWGN(y_wo_noise.transpose(), snr)\n",
    "\n",
    "            y = y_wo_noise + noise.transpose()\n",
    "\n",
    "            DataSet_x.append(x)    # ! I, Q sample distance by Nt.\n",
    "            DataSet_y.append(y)                 # ! output sample\n",
    "            \n",
    "            # Imperfect channel: 5%\n",
    "            DataSet_HH.append(HH)\n",
    "            DataSet_H.append(H)               # ! Generated channel\n",
    "\n",
    "\n",
    "    # Shuffle dataset\n",
    "    random.seed(1)\n",
    "    temp = list(zip(DataSet_x, DataSet_y, DataSet_H, DataSet_HH))\n",
    "    random.shuffle(temp)\n",
    "    DataSet_x, DataSet_y, DataSet_H, DataSet_HH = zip(*temp)\n",
    "\n",
    "    return DataSet_x, DataSet_y, DataSet_H, DataSet_HH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_channel (H):\n",
    "# H_raw = [R(H) I(H); -I(H) R(H)]\n",
    "# we have four version of H_est\n",
    "    H_est_1 = []\n",
    "    H_est_2 = []\n",
    "    H_est_3 = []\n",
    "    H_est_4 = []\n",
    "\n",
    "    H_est_Re_1 = H[0:Nt, 0:Nr]\n",
    "    H_est_Im_1 = H[0:Nt, Nr:2*Nr]\n",
    "    H_est_Im_2 = - H[Nt:2*Nt, 0:Nr]\n",
    "    H_est_Re_2 = H[Nt:2*Nt, Nr:2*Nr]\n",
    "\n",
    "    H_est_1 = H_est_Re_1 + 1j * H_est_Im_1\n",
    "    H_est_2 = H_est_Re_1 + 1j * H_est_Im_2\n",
    "    H_est_3 = H_est_Re_2 + 1j * H_est_Im_1\n",
    "    H_est_4 = H_est_Re_2 + 1j * H_est_Im_2\n",
    "    \n",
    "    return H_est_1, H_est_2, H_est_3, H_est_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMSE(H_est, H_raw):\n",
    "    H_est_1, H_est_2, H_est_3, H_est_4 = reconstruct_channel(H_est)\n",
    "    H_est_vec_1 = torch.reshape(H_est_1, [Nt * Nr, 1])\n",
    "    H_est_vec_2 = torch.reshape(H_est_2, [Nt * Nr, 1])\n",
    "    H_est_vec_3 = torch.reshape(H_est_3, [Nt * Nr, 1])\n",
    "    H_est_vec_4 = torch.reshape(H_est_4, [Nt * Nr, 1])\n",
    "\n",
    "    H_raw_vec = torch.reshape(H_raw, [Nt * Nr, 1])\n",
    "\n",
    "    mse_1       = (torch.norm(H_raw_vec - H_est_vec_1)**2) / len(H_raw_vec)\n",
    "    mse_2       = (torch.norm(H_raw_vec - H_est_vec_2)**2) / len(H_raw_vec)\n",
    "    mse_3       = (torch.norm(H_raw_vec - H_est_vec_3)**2) / len(H_raw_vec)\n",
    "    mse_4       = (torch.norm(H_raw_vec - H_est_vec_4)**2) / len(H_raw_vec)\n",
    "\n",
    "    sigEner   = torch.norm(H_raw_vec)**2\n",
    "\n",
    "    nmse_1      = mse_1 / sigEner\n",
    "    nmse_2      = mse_2 / sigEner\n",
    "    nmse_3      = mse_3 / sigEner\n",
    "    nmse_4      = mse_4 / sigEner\n",
    "\n",
    "    # Best nmse\n",
    "    nmse        = min([nmse_1, nmse_2, nmse_3, nmse_4])\n",
    "\n",
    "    # E = H_raw - H_est\n",
    "    \n",
    "    # # Tính tổng các bình phương của sự khác biệt\n",
    "    # sum_squares_E = torch.sum(E * torch.conj(E))\n",
    "    \n",
    "    # # Tính tổng các bình phương của các phần tử của ma trận H\n",
    "    # sum_squares_H = torch.sum(H_raw * torch.conj(H_raw))\n",
    "    \n",
    "    # # Tính NMSE\n",
    "    # nmse = sum_squares_E / sum_squares_H\n",
    "\n",
    "    return torch.abs(nmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pyjIgxUurU0P"
   },
   "outputs": [],
   "source": [
    "def Input_ISDNN(mode, DataSet_x, DataSet_y, DataSet_H, DataSet_HH, N_samp):\n",
    "    H_in = []        # ! H_in    , np.diag(np.diag()) return a diag matrix instead of diag components.\n",
    "    H_true = []   # ! generated s\n",
    "    H_raw = []\n",
    "    e = []        # ! vector errors\n",
    "    xTx = []\n",
    "    xTy = []\n",
    "    Di = []\n",
    "    # steering = [] # ! Steering vector: ZoA and AoA\n",
    "\n",
    "    if mode == 'train':\n",
    "        n_sample = N_samp * len(SNR)\n",
    "    else:\n",
    "        n_sample = N_samp\n",
    "        \n",
    "    for i in range (n_sample):\n",
    "        H_true.append(torch.tensor(DataSet_HH[i]))\n",
    "        H_raw.append(torch.tensor(DataSet_H[i]))\n",
    "        Di.append(torch.tensor(np.linalg.pinv(np.diag(np.diag(np.dot(DataSet_x[i].transpose(), DataSet_x[i]))))))\n",
    "        xTy.append(torch.tensor(np.dot(DataSet_x[i].transpose(), DataSet_y[i])))\n",
    "        H_in.append(torch.matmul(Di[i], xTy[i]))\n",
    "        e.append(torch.rand([2*Nt, 2*Nr]))\n",
    "        xTx.append(torch.tensor(np.dot(DataSet_x[i].transpose(), DataSet_x[i])))\n",
    "        # steering.append(torch.tensor(DataSet_Steering[i]))\n",
    "\n",
    "    H_true = torch.stack(H_true, dim=0)\n",
    "    H_raw = torch.stack(H_raw, dim=0)\n",
    "    H_in = torch.stack(H_in, dim=0)\n",
    "    e = torch.stack(e, dim=0)\n",
    "    xTx = torch.stack(xTx, dim=0)\n",
    "    xTy = torch.stack(xTy, dim=0)\n",
    "    Di = torch.stack(Di, dim=0)\n",
    "    # steering = torch.stack(steering, dim=0)\n",
    "\n",
    "    return H_true, H_raw, H_in, e, xTx, xTy, Di"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGhdBsghq3M9"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "GHceh5kuq3ZD"
   },
   "outputs": [],
   "source": [
    "class xv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(xv, self).__init__()\n",
    "\n",
    "        self.alpha1 = torch.nn.parameter.Parameter(torch.rand(1))\n",
    "        self.alpha2 = torch.nn.parameter.Parameter(torch.tensor([0.5]))\n",
    "\n",
    "    def forward(self, Di, H, e, xTx, xTy):\n",
    "\n",
    "        xTxH = torch.bmm(xTx, H)\n",
    "\n",
    "        z    = H + torch.bmm(Di, torch.sub(xTy, xTxH)) + self.alpha1 * e\n",
    "\n",
    "        e    = torch.sub(xTy, xTxH)\n",
    "\n",
    "        H    = torch.add((1 - self.alpha2) * z, self.alpha2 * H)\n",
    "\n",
    "        return H, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "woRjD7lJssRq"
   },
   "outputs": [],
   "source": [
    "class model_driven(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model_driven, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(2*Nr, 2*Nr)\n",
    "        self.fc2 = torch.nn.Linear(2*Nr, 2*Nr)\n",
    "        self.fc3 = torch.nn.Linear(2*Nr, 2*Nr)\n",
    "        self.fc4 = torch.nn.Linear(2*Nr, 2*Nr)\n",
    "        self.fc5 = torch.nn.Linear(2*Nr, 2*Nr)\n",
    "        self.fc6 = torch.nn.Linear(2*Nr, 2*Nr)\n",
    "        self.fc7 = torch.nn.Linear(2*Nr, 2*Nr)\n",
    "        self.fc8 = torch.nn.Linear(2*Nr, 2*Nr)\n",
    "\n",
    "        self.layer1=xv()\n",
    "        self.layer2=xv()\n",
    "        self.layer3=xv()\n",
    "        self.layer4=xv()\n",
    "    \n",
    "    def forward(self, Di, H_in, e, xTx, xTy):\n",
    "        e = self.fc1(e)\n",
    "        e = self.fc2(e)\n",
    "\n",
    "        H, e = self.layer1(Di, H_in, e, xTx, xTy)\n",
    "        H = torch.tanh(H)\n",
    "\n",
    "        e = self.fc3(e)\n",
    "        e = self.fc4(e)\n",
    "        H, e = self.layer2(Di, H, e, xTx, xTy)\n",
    "        H = torch.tanh(H)\n",
    "\n",
    "        e = self.fc5(e)\n",
    "        e = self.fc6(e)\n",
    "        H, e = self.layer3(Di, H, e, xTx, xTy)\n",
    "        # H = torch.tanh(H)\n",
    "\n",
    "        # e = self.fc7(e)\n",
    "        # e = self.fc8(e)\n",
    "        # H, e = self.layer4(Di, H, e, xTx, xTy)\n",
    "\n",
    "        return H, e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZjihTOXq3kG"
   },
   "source": [
    "# Define model, optimizer, and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Vp9fRd3gq3tw"
   },
   "outputs": [],
   "source": [
    "def def_model():\n",
    "    model = model_driven()\n",
    "    loss = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    folder_model = './model/'\n",
    "    \n",
    "    if not os.path.isdir(folder_model):\n",
    "        os.makedirs(folder_model)\n",
    "    \n",
    "    file_model = folder_model + 'H'\n",
    "    # if os.path.isfile(file_model):\n",
    "    #     generator = torch.load(file_model)\n",
    "\n",
    "    record_file = 'H'\n",
    "    return model, loss, optimizer, record_file, file_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYWM7SzItKzS"
   },
   "source": [
    "# Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jv7lDwyxtFe3"
   },
   "outputs": [],
   "source": [
    "epoch         = 0\n",
    "expected_epoch = 20000\n",
    "num_samp      = N_samp * len(SNR)\n",
    "best_nmse     = 1e9\n",
    "early_stop    = 0\n",
    "best_model    = ''\n",
    "\n",
    "DataSet_x, DataSet_y, DataSet_H, DataSet_HH = Gen_dataset('train', 0, 0, N_samp)\n",
    "H_true, H_raw, H_in, e, xTx, xTy, Di = Input_ISDNN('train', DataSet_x, DataSet_y, DataSet_H, DataSet_HH, N_samp)\n",
    "\n",
    "print(\"Begin training...\") \n",
    "\n",
    "while(True):\n",
    "        epoch = epoch + 1 \n",
    "\n",
    "        init_loss = 1e9\n",
    "        while( epoch == 1 and init_loss > 130):\n",
    "                \n",
    "                model, loss, optimizer, record_file, file_model = def_model()\n",
    "                \n",
    "                H_1, e_1 = model(Di, H_in, e, xTx, xTy)   # predict output from the model \n",
    "                init_loss = loss(H_1, H_true).item()\n",
    "                print(init_loss)\n",
    "\n",
    "        # optimizer.zero_grad()   # zero the parameter gradients\n",
    "        H_o, e_o = model(Di, H_in, e, xTx, xTy)   # predict output from the model \n",
    "        train_loss = loss(H_o, H_true)   # calculate loss for the predicted output  \n",
    "        train_loss.backward()   # backpropagate the loss \n",
    "        optimizer.step()        # adjust parameters based on the calculated gradients \n",
    "\n",
    "        if (epoch % 100 == 0 or epoch == 1):\n",
    "                nmse = 0\n",
    "                for j in range (num_samp):\n",
    "                        nmse += NMSE(H_o[j], H_raw[j])\n",
    "                nmse = nmse / num_samp\n",
    "                \n",
    "                if (nmse <= best_nmse):\n",
    "                        torch.save(model.state_dict(), file_model + '_' + str(epoch) + '.pth')\n",
    "                        best_model = file_model + '_' + str(epoch) + '.pth'\n",
    "                        best_nmse = nmse\n",
    "                        early_stop = 0\n",
    "                else:\n",
    "                        early_stop += 1\n",
    "\n",
    "                if (nmse > best_nmse and early_stop == 2):\n",
    "                        with Record(record_file + '_log.txt'):\n",
    "                                print(epoch, nmse.item(), train_loss.item()) \n",
    "                        break\n",
    "\n",
    "                with Record(record_file + '_log.txt'):\n",
    "                        print(epoch, nmse.item(), train_loss.item()) \n",
    "\n",
    "        if epoch  == expected_epoch:\n",
    "                torch.save(model.state_dict(), file_model + '_' + str(epoch) + '.pth')\n",
    "                best_model = file_model + '_' + str(epoch) + '.pth'\n",
    "                with Record(record_file + '_log.txt'):\n",
    "                        print(\"epoch:\\n\", epoch)\n",
    "                        print(\"Latest NMSE:\\n\", nmse.item())\n",
    "                        print(\"Latest Loss:\\n\", train_loss.item()) \n",
    "\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOBK-_l-tRMO"
   },
   "source": [
    "# Test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRRFGAX4tg_6"
   },
   "source": [
    "# Function to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SfvpexfwthNq"
   },
   "outputs": [],
   "source": [
    "# from scipy.io import savemat\n",
    "\n",
    "def test(H_raw, Di, H_in, e, xTx, xTy, N_test, log): \n",
    "    # Load the model that we saved at the end of the training loop \n",
    "    model = model_driven()\n",
    "    model.load_state_dict(torch.load(best_model)) \n",
    "      \n",
    "    with torch.no_grad(): \n",
    "        H_o, e_o = model(Di, H_in, e, xTx, xTy)\n",
    "\n",
    "        nmse = 0\n",
    "        for j in range (N_test):\n",
    "                # tmp =  H_o[j]\n",
    "                # tmp1 = tmp.numpy()\n",
    "                # savemat('H_est.mat', {'H_o': tmp1})\n",
    "                nmse += NMSE(H_o[j], H_raw[j])\n",
    "\n",
    "        nmse = nmse / N_test\n",
    "        with Record(log):\n",
    "            print(format(nmse.item(), '.7f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate dataset for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = './model/H_3600.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0030258\n",
      "2.0\n",
      "0.0029800\n",
      "4.0\n",
      "0.0029833\n",
      "6.0\n",
      "0.0030131\n",
      "8.0\n",
      "0.0029454\n",
      "10.0\n",
      "0.0029506\n",
      "12.0\n",
      "0.0029259\n",
      "14.0\n",
      "0.0029287\n",
      "16.0\n",
      "0.0029463\n",
      "18.0\n",
      "0.0029313\n",
      "20.0\n",
      "0.0029701\n",
      "0.0\n",
      "0.0030102\n",
      "2.0\n",
      "0.0030146\n",
      "4.0\n",
      "0.0029748\n",
      "6.0\n",
      "0.0029684\n",
      "8.0\n",
      "0.0029274\n",
      "10.0\n",
      "0.0029835\n",
      "12.0\n",
      "0.0029345\n",
      "14.0\n",
      "0.0029627\n",
      "16.0\n",
      "0.0029503\n",
      "18.0\n",
      "0.0029676\n",
      "20.0\n",
      "0.0029862\n",
      "0.0\n",
      "0.0030375\n",
      "2.0\n",
      "0.0029758\n",
      "4.0\n",
      "0.0029817\n",
      "6.0\n",
      "0.0029878\n",
      "8.0\n",
      "0.0029748\n",
      "10.0\n",
      "0.0029291\n",
      "12.0\n",
      "0.0029398\n",
      "14.0\n",
      "0.0029503\n",
      "16.0\n",
      "0.0029215\n",
      "18.0\n",
      "0.0029012\n",
      "20.0\n",
      "0.0029528\n",
      "0.0\n",
      "0.0030275\n",
      "2.0\n",
      "0.0029656\n",
      "4.0\n",
      "0.0029612\n",
      "6.0\n",
      "0.0029449\n",
      "8.0\n",
      "0.0029511\n",
      "10.0\n",
      "0.0029150\n",
      "12.0\n",
      "0.0029516\n",
      "14.0\n",
      "0.0029525\n",
      "16.0\n",
      "0.0029844\n",
      "18.0\n",
      "0.0029732\n",
      "20.0\n",
      "0.0029401\n",
      "0.0\n",
      "0.0030072\n",
      "2.0\n",
      "0.0029749\n",
      "4.0\n",
      "0.0030256\n",
      "6.0\n",
      "0.0029555\n",
      "8.0\n",
      "0.0029752\n",
      "10.0\n",
      "0.0029719\n",
      "12.0\n",
      "0.0029527\n",
      "14.0\n",
      "0.0029379\n",
      "16.0\n",
      "0.0029682\n",
      "18.0\n",
      "0.0029528\n",
      "20.0\n",
      "0.0030118\n",
      "0.0\n",
      "0.0030421\n",
      "2.0\n",
      "0.0029957\n",
      "4.0\n",
      "0.0030000\n",
      "6.0\n",
      "0.0029807\n",
      "8.0\n",
      "0.0029477\n",
      "10.0\n",
      "0.0029413\n",
      "12.0\n",
      "0.0029704\n",
      "14.0\n",
      "0.0029160\n",
      "16.0\n",
      "0.0029222\n",
      "18.0\n",
      "0.0029268\n",
      "20.0\n",
      "0.0029265\n",
      "0.0\n",
      "0.0030386\n",
      "2.0\n",
      "0.0030218\n",
      "4.0\n",
      "0.0029814\n",
      "6.0\n",
      "0.0029466\n",
      "8.0\n",
      "0.0029502\n",
      "10.0\n",
      "0.0029489\n",
      "12.0\n",
      "0.0029470\n",
      "14.0\n",
      "0.0029675\n",
      "16.0\n",
      "0.0029526\n",
      "18.0\n",
      "0.0029282\n",
      "20.0\n",
      "0.0029881\n",
      "0.0\n",
      "0.0029798\n",
      "2.0\n",
      "0.0029840\n",
      "4.0\n",
      "0.0029764\n",
      "6.0\n",
      "0.0029714\n",
      "8.0\n",
      "0.0029425\n",
      "10.0\n",
      "0.0029785\n",
      "12.0\n",
      "0.0029257\n",
      "14.0\n",
      "0.0029575\n",
      "16.0\n",
      "0.0029846\n",
      "18.0\n",
      "0.0029355\n",
      "20.0\n",
      "0.0029256\n",
      "0.0\n",
      "0.0030046\n",
      "2.0\n",
      "0.0030033\n",
      "4.0\n",
      "0.0029986\n",
      "6.0\n",
      "0.0029766\n",
      "8.0\n",
      "0.0029420\n",
      "10.0\n",
      "0.0029758\n",
      "12.0\n",
      "0.0029034\n",
      "14.0\n",
      "0.0029444\n",
      "16.0\n",
      "0.0029697\n",
      "18.0\n",
      "0.0029336\n",
      "20.0\n",
      "0.0029669\n",
      "0.0\n",
      "0.0030844\n",
      "2.0\n",
      "0.0030116\n",
      "4.0\n",
      "0.0029623\n",
      "6.0\n",
      "0.0029706\n",
      "8.0\n",
      "0.0029413\n",
      "10.0\n",
      "0.0029153\n",
      "12.0\n",
      "0.0029462\n",
      "14.0\n",
      "0.0029289\n",
      "16.0\n",
      "0.0029174\n",
      "18.0\n",
      "0.0029357\n",
      "20.0\n",
      "0.0029433\n",
      "0.0\n",
      "0.0029983\n",
      "2.0\n",
      "0.0030233\n",
      "4.0\n",
      "0.0029941\n",
      "6.0\n",
      "0.0029935\n",
      "8.0\n",
      "0.0029102\n",
      "10.0\n",
      "0.0030051\n",
      "12.0\n",
      "0.0029423\n",
      "14.0\n",
      "0.0028931\n",
      "16.0\n",
      "0.0029743\n",
      "18.0\n",
      "0.0029277\n",
      "20.0\n",
      "0.0029508\n",
      "0.0\n",
      "0.0030636\n",
      "2.0\n",
      "0.0030010\n",
      "4.0\n",
      "0.0029672\n",
      "6.0\n",
      "0.0029879\n",
      "8.0\n",
      "0.0029460\n",
      "10.0\n",
      "0.0029505\n",
      "12.0\n",
      "0.0029485\n",
      "14.0\n",
      "0.0029539\n",
      "16.0\n",
      "0.0029659\n",
      "18.0\n",
      "0.0029224\n",
      "20.0\n",
      "0.0029611\n",
      "0.0\n",
      "0.0029824\n",
      "2.0\n",
      "0.0030185\n",
      "4.0\n",
      "0.0029524\n",
      "6.0\n",
      "0.0029593\n",
      "8.0\n",
      "0.0029566\n",
      "10.0\n",
      "0.0029553\n",
      "12.0\n",
      "0.0029350\n",
      "14.0\n",
      "0.0029697\n",
      "16.0\n",
      "0.0029575\n",
      "18.0\n",
      "0.0029701\n",
      "20.0\n",
      "0.0029101\n",
      "0.0\n",
      "0.0030269\n",
      "2.0\n",
      "0.0030206\n",
      "4.0\n",
      "0.0029478\n",
      "6.0\n",
      "0.0029374\n",
      "8.0\n",
      "0.0029610\n",
      "10.0\n",
      "0.0029489\n",
      "12.0\n",
      "0.0029430\n",
      "14.0\n",
      "0.0029190\n",
      "16.0\n",
      "0.0029368\n",
      "18.0\n",
      "0.0029234\n",
      "20.0\n",
      "0.0029593\n",
      "0.0\n",
      "0.0030633\n",
      "2.0\n",
      "0.0029661\n",
      "4.0\n",
      "0.0029692\n",
      "6.0\n",
      "0.0030218\n",
      "8.0\n",
      "0.0029476\n",
      "10.0\n",
      "0.0029856\n",
      "12.0\n",
      "0.0029435\n",
      "14.0\n",
      "0.0029278\n",
      "16.0\n",
      "0.0029405\n",
      "18.0\n",
      "0.0029511\n",
      "20.0\n",
      "0.0029500\n",
      "0.0\n",
      "0.0030098\n",
      "2.0\n",
      "0.0030520\n",
      "4.0\n",
      "0.0029844\n",
      "6.0\n",
      "0.0029492\n",
      "8.0\n",
      "0.0029602\n",
      "10.0\n",
      "0.0029684\n",
      "12.0\n",
      "0.0029520\n",
      "14.0\n",
      "0.0029889\n",
      "16.0\n",
      "0.0029699\n",
      "18.0\n",
      "0.0029598\n",
      "20.0\n",
      "0.0029406\n",
      "0.0\n",
      "0.0030279\n",
      "2.0\n",
      "0.0029852\n",
      "4.0\n",
      "0.0029921\n",
      "6.0\n",
      "0.0029888\n",
      "8.0\n",
      "0.0029798\n",
      "10.0\n",
      "0.0029641\n",
      "12.0\n",
      "0.0029810\n",
      "14.0\n",
      "0.0029578\n",
      "16.0\n",
      "0.0029788\n",
      "18.0\n",
      "0.0029364\n",
      "20.0\n",
      "0.0029611\n",
      "0.0\n",
      "0.0030077\n",
      "2.0\n",
      "0.0030049\n",
      "4.0\n",
      "0.0029940\n",
      "6.0\n",
      "0.0029304\n",
      "8.0\n",
      "0.0029478\n",
      "10.0\n",
      "0.0029527\n",
      "12.0\n",
      "0.0029700\n",
      "14.0\n",
      "0.0029407\n",
      "16.0\n",
      "0.0029511\n",
      "18.0\n",
      "0.0029323\n",
      "20.0\n",
      "0.0029613\n",
      "0.0\n",
      "0.0030587\n",
      "2.0\n",
      "0.0030009\n",
      "4.0\n",
      "0.0029867\n",
      "6.0\n",
      "0.0029448\n",
      "8.0\n",
      "0.0029347\n",
      "10.0\n",
      "0.0029589\n",
      "12.0\n",
      "0.0029156\n",
      "14.0\n",
      "0.0029627\n",
      "16.0\n",
      "0.0029225\n",
      "18.0\n",
      "0.0029704\n",
      "20.0\n",
      "0.0029477\n",
      "0.0\n",
      "0.0029662\n",
      "2.0\n",
      "0.0029936\n",
      "4.0\n",
      "0.0029904\n",
      "6.0\n",
      "0.0029666\n",
      "8.0\n",
      "0.0029460\n",
      "10.0\n",
      "0.0029712\n",
      "12.0\n",
      "0.0029620\n",
      "14.0\n",
      "0.0029370\n",
      "16.0\n",
      "0.0029329\n",
      "18.0\n",
      "0.0029646\n",
      "20.0\n",
      "0.0029708\n",
      "0.0\n",
      "0.0030144\n",
      "2.0\n",
      "0.0029913\n",
      "4.0\n",
      "0.0029601\n",
      "6.0\n",
      "0.0029725\n",
      "8.0\n",
      "0.0029651\n",
      "10.0\n",
      "0.0029377\n",
      "12.0\n",
      "0.0029895\n",
      "14.0\n",
      "0.0029825\n",
      "16.0\n",
      "0.0029851\n",
      "18.0\n",
      "0.0029464\n",
      "20.0\n",
      "0.0029039\n",
      "0.0\n",
      "0.0030098\n",
      "2.0\n",
      "0.0029327\n",
      "4.0\n",
      "0.0029883\n",
      "6.0\n",
      "0.0029609\n",
      "8.0\n",
      "0.0029750\n",
      "10.0\n",
      "0.0029489\n",
      "12.0\n",
      "0.0029307\n",
      "14.0\n",
      "0.0029825\n",
      "16.0\n",
      "0.0029126\n",
      "18.0\n",
      "0.0029323\n",
      "20.0\n",
      "0.0029542\n",
      "0.0\n",
      "0.0030435\n",
      "2.0\n",
      "0.0030137\n",
      "4.0\n",
      "0.0029673\n",
      "6.0\n",
      "0.0029254\n",
      "8.0\n",
      "0.0030245\n",
      "10.0\n",
      "0.0029455\n",
      "12.0\n",
      "0.0029562\n",
      "14.0\n",
      "0.0029717\n",
      "16.0\n",
      "0.0029493\n",
      "18.0\n",
      "0.0029559\n",
      "20.0\n",
      "0.0029373\n",
      "0.0\n",
      "0.0030705\n",
      "2.0\n",
      "0.0029397\n",
      "4.0\n",
      "0.0029988\n",
      "6.0\n",
      "0.0029865\n",
      "8.0\n",
      "0.0029692\n",
      "10.0\n",
      "0.0029202\n",
      "12.0\n",
      "0.0029726\n",
      "14.0\n",
      "0.0029596\n",
      "16.0\n",
      "0.0029283\n",
      "18.0\n",
      "0.0029652\n",
      "20.0\n",
      "0.0029540\n",
      "0.0\n",
      "0.0030268\n",
      "2.0\n",
      "0.0029914\n",
      "4.0\n",
      "0.0029758\n",
      "6.0\n",
      "0.0029970\n",
      "8.0\n",
      "0.0029730\n",
      "10.0\n",
      "0.0029589\n",
      "12.0\n",
      "0.0029497\n",
      "14.0\n",
      "0.0029489\n",
      "16.0\n",
      "0.0029316\n",
      "18.0\n",
      "0.0029377\n",
      "20.0\n",
      "0.0029224\n",
      "0.0\n",
      "0.0030031\n",
      "2.0\n",
      "0.0030364\n",
      "4.0\n",
      "0.0029474\n",
      "6.0\n",
      "0.0029935\n",
      "8.0\n",
      "0.0029838\n",
      "10.0\n",
      "0.0029780\n",
      "12.0\n",
      "0.0029800\n",
      "14.0\n",
      "0.0029418\n",
      "16.0\n",
      "0.0029208\n",
      "18.0\n",
      "0.0029506\n",
      "20.0\n",
      "0.0029646\n",
      "0.0\n",
      "0.0029984\n",
      "2.0\n",
      "0.0030039\n",
      "4.0\n",
      "0.0029972\n",
      "6.0\n",
      "0.0029452\n",
      "8.0\n",
      "0.0029468\n",
      "10.0\n",
      "0.0029625\n",
      "12.0\n",
      "0.0029849\n",
      "14.0\n",
      "0.0029626\n",
      "16.0\n",
      "0.0029061\n",
      "18.0\n",
      "0.0029463\n",
      "20.0\n",
      "0.0029224\n",
      "0.0\n",
      "0.0030370\n",
      "2.0\n",
      "0.0029949\n",
      "4.0\n",
      "0.0029064\n",
      "6.0\n",
      "0.0029778\n",
      "8.0\n",
      "0.0029875\n",
      "10.0\n",
      "0.0029770\n",
      "12.0\n",
      "0.0029730\n",
      "14.0\n",
      "0.0029626\n",
      "16.0\n",
      "0.0029719\n",
      "18.0\n",
      "0.0029498\n",
      "20.0\n",
      "0.0029466\n",
      "0.0\n",
      "0.0029970\n",
      "2.0\n",
      "0.0029663\n",
      "4.0\n",
      "0.0029717\n",
      "6.0\n",
      "0.0029663\n",
      "8.0\n",
      "0.0029616\n",
      "10.0\n",
      "0.0029541\n",
      "12.0\n",
      "0.0029478\n",
      "14.0\n",
      "0.0029683\n",
      "16.0\n",
      "0.0029368\n",
      "18.0\n",
      "0.0029411\n",
      "20.0\n",
      "0.0029509\n",
      "0.0\n",
      "0.0030363\n",
      "2.0\n",
      "0.0029713\n",
      "4.0\n",
      "0.0029753\n",
      "6.0\n",
      "0.0029324\n",
      "8.0\n",
      "0.0029993\n",
      "10.0\n",
      "0.0029501\n",
      "12.0\n",
      "0.0029524\n",
      "14.0\n",
      "0.0029726\n",
      "16.0\n",
      "0.0029644\n",
      "18.0\n",
      "0.0029228\n",
      "20.0\n",
      "0.0029328\n",
      "0.0\n",
      "0.0029977\n",
      "2.0\n",
      "0.0030037\n",
      "4.0\n",
      "0.0029863\n",
      "6.0\n",
      "0.0029429\n",
      "8.0\n",
      "0.0029505\n",
      "10.0\n",
      "0.0029665\n",
      "12.0\n",
      "0.0029448\n",
      "14.0\n",
      "0.0029618\n",
      "16.0\n",
      "0.0029463\n",
      "18.0\n",
      "0.0029302\n",
      "20.0\n",
      "0.0028999\n",
      "0.0\n",
      "0.0030327\n",
      "2.0\n",
      "0.0029791\n",
      "4.0\n",
      "0.0029969\n",
      "6.0\n",
      "0.0029696\n",
      "8.0\n",
      "0.0029610\n",
      "10.0\n",
      "0.0029847\n",
      "12.0\n",
      "0.0029635\n",
      "14.0\n",
      "0.0029770\n",
      "16.0\n",
      "0.0029669\n",
      "18.0\n",
      "0.0029528\n",
      "20.0\n",
      "0.0029533\n",
      "0.0\n",
      "0.0030560\n",
      "2.0\n",
      "0.0029739\n",
      "4.0\n",
      "0.0030033\n",
      "6.0\n",
      "0.0029766\n",
      "8.0\n",
      "0.0030011\n",
      "10.0\n",
      "0.0029128\n",
      "12.0\n",
      "0.0029611\n",
      "14.0\n",
      "0.0029355\n",
      "16.0\n",
      "0.0029488\n",
      "18.0\n",
      "0.0029313\n",
      "20.0\n",
      "0.0029403\n",
      "0.0\n",
      "0.0030012\n",
      "2.0\n",
      "0.0030252\n",
      "4.0\n",
      "0.0029781\n",
      "6.0\n",
      "0.0029976\n",
      "8.0\n",
      "0.0029775\n",
      "10.0\n",
      "0.0029771\n",
      "12.0\n",
      "0.0029731\n",
      "14.0\n",
      "0.0029772\n",
      "16.0\n",
      "0.0029388\n",
      "18.0\n",
      "0.0029194\n",
      "20.0\n",
      "0.0029906\n",
      "0.0\n",
      "0.0030167\n",
      "2.0\n",
      "0.0029952\n",
      "4.0\n",
      "0.0029974\n",
      "6.0\n",
      "0.0029702\n",
      "8.0\n",
      "0.0029656\n",
      "10.0\n",
      "0.0029074\n",
      "12.0\n",
      "0.0029583\n",
      "14.0\n",
      "0.0029334\n",
      "16.0\n",
      "0.0029500\n",
      "18.0\n",
      "0.0029543\n",
      "20.0\n",
      "0.0029842\n",
      "0.0\n",
      "0.0029957\n",
      "2.0\n",
      "0.0029851\n",
      "4.0\n",
      "0.0029683\n",
      "6.0\n",
      "0.0029555\n",
      "8.0\n",
      "0.0029439\n",
      "10.0\n",
      "0.0029651\n",
      "12.0\n",
      "0.0029775\n",
      "14.0\n",
      "0.0029109\n",
      "16.0\n",
      "0.0029209\n",
      "18.0\n",
      "0.0029761\n",
      "20.0\n",
      "0.0029181\n",
      "0.0\n",
      "0.0030116\n",
      "2.0\n",
      "0.0029907\n",
      "4.0\n",
      "0.0029567\n",
      "6.0\n",
      "0.0029760\n",
      "8.0\n",
      "0.0029509\n",
      "10.0\n",
      "0.0029515\n",
      "12.0\n",
      "0.0029275\n",
      "14.0\n",
      "0.0029399\n",
      "16.0\n",
      "0.0029232\n",
      "18.0\n",
      "0.0029157\n",
      "20.0\n",
      "0.0029525\n",
      "0.0\n",
      "0.0030186\n",
      "2.0\n",
      "0.0030189\n",
      "4.0\n",
      "0.0029802\n",
      "6.0\n",
      "0.0029468\n",
      "8.0\n",
      "0.0029391\n",
      "10.0\n",
      "0.0029713\n",
      "12.0\n",
      "0.0029218\n",
      "14.0\n",
      "0.0029474\n",
      "16.0\n",
      "0.0029396\n",
      "18.0\n",
      "0.0029107\n",
      "20.0\n",
      "0.0029117\n",
      "0.0\n",
      "0.0030086\n",
      "2.0\n",
      "0.0029477\n",
      "4.0\n",
      "0.0030076\n",
      "6.0\n",
      "0.0029385\n",
      "8.0\n",
      "0.0029616\n",
      "10.0\n",
      "0.0029444\n",
      "12.0\n",
      "0.0029730\n",
      "14.0\n",
      "0.0029420\n",
      "16.0\n",
      "0.0029503\n",
      "18.0\n",
      "0.0029515\n",
      "20.0\n",
      "0.0029225\n",
      "0.0\n",
      "0.0030334\n",
      "2.0\n",
      "0.0029904\n",
      "4.0\n",
      "0.0029625\n",
      "6.0\n",
      "0.0029686\n",
      "8.0\n",
      "0.0029511\n",
      "10.0\n",
      "0.0029461\n",
      "12.0\n",
      "0.0029509\n",
      "14.0\n",
      "0.0029453\n",
      "16.0\n",
      "0.0029864\n",
      "18.0\n",
      "0.0029362\n",
      "20.0\n",
      "0.0029982\n",
      "0.0\n",
      "0.0030233\n",
      "2.0\n",
      "0.0029871\n",
      "4.0\n",
      "0.0030003\n",
      "6.0\n",
      "0.0029748\n",
      "8.0\n",
      "0.0029747\n",
      "10.0\n",
      "0.0029331\n",
      "12.0\n",
      "0.0029608\n",
      "14.0\n",
      "0.0029588\n",
      "16.0\n",
      "0.0029242\n",
      "18.0\n",
      "0.0029542\n",
      "20.0\n",
      "0.0030164\n",
      "0.0\n",
      "0.0030158\n",
      "2.0\n",
      "0.0030121\n",
      "4.0\n",
      "0.0029683\n",
      "6.0\n",
      "0.0029932\n",
      "8.0\n",
      "0.0029801\n",
      "10.0\n",
      "0.0029561\n",
      "12.0\n",
      "0.0029509\n",
      "14.0\n",
      "0.0029381\n",
      "16.0\n",
      "0.0029482\n",
      "18.0\n",
      "0.0029952\n",
      "20.0\n",
      "0.0029285\n",
      "0.0\n",
      "0.0030415\n",
      "2.0\n",
      "0.0030033\n",
      "4.0\n",
      "0.0029843\n",
      "6.0\n",
      "0.0029644\n",
      "8.0\n",
      "0.0029451\n",
      "10.0\n",
      "0.0029327\n",
      "12.0\n",
      "0.0029002\n",
      "14.0\n",
      "0.0029461\n",
      "16.0\n",
      "0.0029384\n",
      "18.0\n",
      "0.0029513\n",
      "20.0\n",
      "0.0029366\n",
      "0.0\n",
      "0.0029859\n",
      "2.0\n",
      "0.0029862\n",
      "4.0\n",
      "0.0029667\n",
      "6.0\n",
      "0.0029922\n",
      "8.0\n",
      "0.0029567\n",
      "10.0\n",
      "0.0029152\n",
      "12.0\n",
      "0.0029471\n",
      "14.0\n",
      "0.0029159\n",
      "16.0\n",
      "0.0029324\n",
      "18.0\n",
      "0.0029392\n",
      "20.0\n",
      "0.0029124\n",
      "0.0\n",
      "0.0030036\n",
      "2.0\n",
      "0.0029762\n",
      "4.0\n",
      "0.0029993\n",
      "6.0\n",
      "0.0029652\n",
      "8.0\n",
      "0.0029246\n",
      "10.0\n",
      "0.0029434\n",
      "12.0\n",
      "0.0029632\n",
      "14.0\n",
      "0.0029524\n",
      "16.0\n",
      "0.0029740\n",
      "18.0\n",
      "0.0029281\n",
      "20.0\n",
      "0.0029609\n",
      "0.0\n",
      "0.0029537\n",
      "2.0\n",
      "0.0030072\n",
      "4.0\n",
      "0.0029930\n",
      "6.0\n",
      "0.0029530\n",
      "8.0\n",
      "0.0029801\n",
      "10.0\n",
      "0.0030012\n",
      "12.0\n",
      "0.0029730\n",
      "14.0\n",
      "0.0029379\n",
      "16.0\n",
      "0.0029670\n",
      "18.0\n",
      "0.0029381\n",
      "20.0\n",
      "0.0029365\n",
      "0.0\n",
      "0.0029870\n",
      "2.0\n",
      "0.0029585\n",
      "4.0\n",
      "0.0029800\n",
      "6.0\n",
      "0.0029540\n",
      "8.0\n",
      "0.0029592\n",
      "10.0\n",
      "0.0029244\n",
      "12.0\n",
      "0.0029562\n",
      "14.0\n",
      "0.0029096\n",
      "16.0\n",
      "0.0028933\n",
      "18.0\n",
      "0.0029608\n",
      "20.0\n",
      "0.0029197\n",
      "0.0\n",
      "0.0030275\n",
      "2.0\n",
      "0.0030142\n",
      "4.0\n",
      "0.0030048\n",
      "6.0\n",
      "0.0029915\n",
      "8.0\n",
      "0.0029453\n",
      "10.0\n",
      "0.0029662\n",
      "12.0\n",
      "0.0029634\n",
      "14.0\n",
      "0.0029727\n",
      "16.0\n",
      "0.0029403\n",
      "18.0\n",
      "0.0029510\n",
      "20.0\n",
      "0.0029611\n",
      "0.0\n",
      "0.0030638\n",
      "2.0\n",
      "0.0029824\n",
      "4.0\n",
      "0.0029671\n",
      "6.0\n",
      "0.0029541\n",
      "8.0\n",
      "0.0030000\n",
      "10.0\n",
      "0.0029531\n",
      "12.0\n",
      "0.0029238\n",
      "14.0\n",
      "0.0028983\n",
      "16.0\n",
      "0.0029541\n",
      "18.0\n",
      "0.0029434\n",
      "20.0\n",
      "0.0029710\n",
      "0.0\n",
      "0.0030200\n",
      "2.0\n",
      "0.0029945\n",
      "4.0\n",
      "0.0029631\n",
      "6.0\n",
      "0.0029939\n",
      "8.0\n",
      "0.0030027\n",
      "10.0\n",
      "0.0029335\n",
      "12.0\n",
      "0.0029417\n",
      "14.0\n",
      "0.0029713\n",
      "16.0\n",
      "0.0029562\n",
      "18.0\n",
      "0.0029699\n",
      "20.0\n",
      "0.0029528\n",
      "0.0\n",
      "0.0029874\n",
      "2.0\n",
      "0.0029868\n",
      "4.0\n",
      "0.0029792\n",
      "6.0\n",
      "0.0029653\n",
      "8.0\n",
      "0.0029483\n",
      "10.0\n",
      "0.0029971\n",
      "12.0\n",
      "0.0029323\n",
      "14.0\n",
      "0.0029580\n",
      "16.0\n",
      "0.0029599\n",
      "18.0\n",
      "0.0029546\n",
      "20.0\n",
      "0.0029524\n",
      "0.0\n",
      "0.0030099\n",
      "2.0\n",
      "0.0029940\n",
      "4.0\n",
      "0.0029881\n",
      "6.0\n",
      "0.0029574\n",
      "8.0\n",
      "0.0029479\n",
      "10.0\n",
      "0.0029789\n",
      "12.0\n",
      "0.0029259\n",
      "14.0\n",
      "0.0029421\n",
      "16.0\n",
      "0.0029936\n",
      "18.0\n",
      "0.0029629\n",
      "20.0\n",
      "0.0029216\n",
      "0.0\n",
      "0.0030671\n",
      "2.0\n",
      "0.0029985\n",
      "4.0\n",
      "0.0029810\n",
      "6.0\n",
      "0.0029468\n",
      "8.0\n",
      "0.0029140\n",
      "10.0\n",
      "0.0029695\n",
      "12.0\n",
      "0.0029241\n",
      "14.0\n",
      "0.0029749\n",
      "16.0\n",
      "0.0029517\n",
      "18.0\n",
      "0.0029417\n",
      "20.0\n",
      "0.0029376\n",
      "0.0\n",
      "0.0029799\n",
      "2.0\n",
      "0.0029973\n",
      "4.0\n",
      "0.0029672\n",
      "6.0\n",
      "0.0029467\n",
      "8.0\n",
      "0.0029117\n",
      "10.0\n",
      "0.0029619\n",
      "12.0\n",
      "0.0029350\n",
      "14.0\n",
      "0.0029413\n",
      "16.0\n",
      "0.0029436\n",
      "18.0\n",
      "0.0029672\n",
      "20.0\n",
      "0.0029320\n",
      "0.0\n",
      "0.0029786\n",
      "2.0\n",
      "0.0029869\n",
      "4.0\n",
      "0.0029746\n",
      "6.0\n",
      "0.0029554\n",
      "8.0\n",
      "0.0029942\n",
      "10.0\n",
      "0.0029756\n",
      "12.0\n",
      "0.0030208\n",
      "14.0\n",
      "0.0029368\n",
      "16.0\n",
      "0.0029341\n",
      "18.0\n",
      "0.0029766\n",
      "20.0\n",
      "0.0029474\n",
      "0.0\n",
      "0.0030283\n",
      "2.0\n",
      "0.0029876\n",
      "4.0\n",
      "0.0029476\n",
      "6.0\n",
      "0.0029520\n",
      "8.0\n",
      "0.0029592\n",
      "10.0\n",
      "0.0029778\n",
      "12.0\n",
      "0.0029752\n",
      "14.0\n",
      "0.0028947\n",
      "16.0\n",
      "0.0029329\n",
      "18.0\n",
      "0.0029505\n",
      "20.0\n",
      "0.0029071\n",
      "0.0\n",
      "0.0030133\n",
      "2.0\n",
      "0.0029849\n",
      "4.0\n",
      "0.0030032\n",
      "6.0\n",
      "0.0029805\n",
      "8.0\n",
      "0.0029571\n",
      "10.0\n",
      "0.0029344\n",
      "12.0\n",
      "0.0029139\n",
      "14.0\n",
      "0.0029793\n",
      "16.0\n",
      "0.0029402\n",
      "18.0\n",
      "0.0029435\n",
      "20.0\n",
      "0.0029410\n",
      "0.0\n",
      "0.0030411\n",
      "2.0\n",
      "0.0029973\n",
      "4.0\n",
      "0.0029701\n",
      "6.0\n",
      "0.0029559\n",
      "8.0\n",
      "0.0029711\n",
      "10.0\n",
      "0.0029527\n",
      "12.0\n",
      "0.0029445\n",
      "14.0\n",
      "0.0029414\n",
      "16.0\n",
      "0.0029725\n",
      "18.0\n",
      "0.0029747\n",
      "20.0\n",
      "0.0029590\n",
      "0.0\n",
      "0.0030711\n",
      "2.0\n",
      "0.0029743\n",
      "4.0\n",
      "0.0029493\n",
      "6.0\n",
      "0.0029642\n",
      "8.0\n",
      "0.0029863\n",
      "10.0\n",
      "0.0029790\n",
      "12.0\n",
      "0.0029459\n",
      "14.0\n",
      "0.0029747\n",
      "16.0\n",
      "0.0029407\n",
      "18.0\n",
      "0.0029392\n",
      "20.0\n",
      "0.0028987\n",
      "0.0\n",
      "0.0030437\n",
      "2.0\n",
      "0.0030007\n",
      "4.0\n",
      "0.0029900\n",
      "6.0\n",
      "0.0029956\n",
      "8.0\n",
      "0.0029511\n",
      "10.0\n",
      "0.0029529\n",
      "12.0\n",
      "0.0029428\n",
      "14.0\n",
      "0.0029486\n",
      "16.0\n",
      "0.0029818\n",
      "18.0\n",
      "0.0029560\n",
      "20.0\n",
      "0.0029268\n",
      "0.0\n",
      "0.0029952\n",
      "2.0\n",
      "0.0029749\n",
      "4.0\n",
      "0.0029484\n",
      "6.0\n",
      "0.0029466\n",
      "8.0\n",
      "0.0029708\n",
      "10.0\n",
      "0.0029934\n",
      "12.0\n",
      "0.0029606\n",
      "14.0\n",
      "0.0029092\n",
      "16.0\n",
      "0.0029329\n",
      "18.0\n",
      "0.0029438\n",
      "20.0\n",
      "0.0029449\n",
      "0.0\n",
      "0.0029962\n",
      "2.0\n",
      "0.0029938\n",
      "4.0\n",
      "0.0029644\n",
      "6.0\n",
      "0.0029544\n",
      "8.0\n",
      "0.0029447\n",
      "10.0\n",
      "0.0029652\n",
      "12.0\n",
      "0.0029707\n",
      "14.0\n",
      "0.0029267\n",
      "16.0\n",
      "0.0029527\n",
      "18.0\n",
      "0.0029539\n",
      "20.0\n",
      "0.0029622\n",
      "0.0\n",
      "0.0030072\n",
      "2.0\n",
      "0.0029727\n",
      "4.0\n",
      "0.0029718\n",
      "6.0\n",
      "0.0029673\n",
      "8.0\n",
      "0.0029499\n",
      "10.0\n",
      "0.0029596\n",
      "12.0\n",
      "0.0029500\n",
      "14.0\n",
      "0.0029178\n",
      "16.0\n",
      "0.0029379\n",
      "18.0\n",
      "0.0029862\n",
      "20.0\n",
      "0.0029565\n",
      "0.0\n",
      "0.0030236\n",
      "2.0\n",
      "0.0030069\n",
      "4.0\n",
      "0.0030108\n",
      "6.0\n",
      "0.0029463\n",
      "8.0\n",
      "0.0029827\n",
      "10.0\n",
      "0.0029310\n",
      "12.0\n",
      "0.0029812\n",
      "14.0\n",
      "0.0029086\n",
      "16.0\n",
      "0.0029783\n",
      "18.0\n",
      "0.0029381\n",
      "20.0\n",
      "0.0029809\n",
      "0.0\n",
      "0.0030214\n",
      "2.0\n",
      "0.0030127\n",
      "4.0\n",
      "0.0029729\n",
      "6.0\n",
      "0.0029461\n",
      "8.0\n",
      "0.0029909\n",
      "10.0\n",
      "0.0029426\n",
      "12.0\n",
      "0.0029575\n",
      "14.0\n",
      "0.0029362\n",
      "16.0\n",
      "0.0029247\n",
      "18.0\n",
      "0.0029682\n",
      "20.0\n",
      "0.0029164\n",
      "0.0\n",
      "0.0030173\n",
      "2.0\n",
      "0.0030320\n",
      "4.0\n",
      "0.0029208\n",
      "6.0\n",
      "0.0030133\n",
      "8.0\n",
      "0.0029971\n",
      "10.0\n",
      "0.0029674\n",
      "12.0\n",
      "0.0029915\n",
      "14.0\n",
      "0.0029533\n",
      "16.0\n",
      "0.0029749\n",
      "18.0\n",
      "0.0029126\n",
      "20.0\n",
      "0.0029662\n",
      "0.0\n",
      "0.0030151\n",
      "2.0\n",
      "0.0029959\n",
      "4.0\n",
      "0.0029824\n",
      "6.0\n",
      "0.0029403\n",
      "8.0\n",
      "0.0029398\n",
      "10.0\n",
      "0.0029528\n",
      "12.0\n",
      "0.0029402\n",
      "14.0\n",
      "0.0029161\n",
      "16.0\n",
      "0.0029306\n",
      "18.0\n",
      "0.0029446\n",
      "20.0\n",
      "0.0029272\n",
      "0.0\n",
      "0.0029898\n",
      "2.0\n",
      "0.0030453\n",
      "4.0\n",
      "0.0030010\n",
      "6.0\n",
      "0.0029412\n",
      "8.0\n",
      "0.0029353\n",
      "10.0\n",
      "0.0029507\n",
      "12.0\n",
      "0.0029480\n",
      "14.0\n",
      "0.0029724\n",
      "16.0\n",
      "0.0029681\n",
      "18.0\n",
      "0.0029446\n",
      "20.0\n",
      "0.0029540\n",
      "0.0\n",
      "0.0030364\n",
      "2.0\n",
      "0.0030204\n",
      "4.0\n",
      "0.0029545\n",
      "6.0\n",
      "0.0029279\n",
      "8.0\n",
      "0.0029686\n",
      "10.0\n",
      "0.0029018\n",
      "12.0\n",
      "0.0029420\n",
      "14.0\n",
      "0.0029524\n",
      "16.0\n",
      "0.0029631\n",
      "18.0\n",
      "0.0029488\n",
      "20.0\n",
      "0.0029635\n",
      "0.0\n",
      "0.0030111\n",
      "2.0\n",
      "0.0029858\n",
      "4.0\n",
      "0.0029988\n",
      "6.0\n",
      "0.0029652\n",
      "8.0\n",
      "0.0029987\n",
      "10.0\n",
      "0.0029679\n",
      "12.0\n",
      "0.0029303\n",
      "14.0\n",
      "0.0029486\n",
      "16.0\n",
      "0.0029522\n",
      "18.0\n",
      "0.0029836\n",
      "20.0\n",
      "0.0029605\n",
      "0.0\n",
      "0.0030347\n",
      "2.0\n",
      "0.0030193\n",
      "4.0\n",
      "0.0029463\n",
      "6.0\n",
      "0.0029866\n",
      "8.0\n",
      "0.0029436\n",
      "10.0\n",
      "0.0029574\n",
      "12.0\n",
      "0.0030129\n",
      "14.0\n",
      "0.0028864\n",
      "16.0\n",
      "0.0029201\n",
      "18.0\n",
      "0.0029530\n",
      "20.0\n",
      "0.0028999\n",
      "0.0\n",
      "0.0030312\n",
      "2.0\n",
      "0.0029881\n",
      "4.0\n",
      "0.0029743\n",
      "6.0\n",
      "0.0029862\n",
      "8.0\n",
      "0.0029303\n",
      "10.0\n",
      "0.0029569\n",
      "12.0\n",
      "0.0029487\n",
      "14.0\n",
      "0.0030043\n",
      "16.0\n",
      "0.0029437\n",
      "18.0\n",
      "0.0029676\n",
      "20.0\n",
      "0.0029699\n",
      "0.0\n",
      "0.0030244\n",
      "2.0\n",
      "0.0030053\n",
      "4.0\n",
      "0.0029684\n",
      "6.0\n",
      "0.0029936\n",
      "8.0\n",
      "0.0029762\n",
      "10.0\n",
      "0.0029538\n",
      "12.0\n",
      "0.0029196\n",
      "14.0\n",
      "0.0029473\n",
      "16.0\n",
      "0.0029478\n",
      "18.0\n",
      "0.0029178\n",
      "20.0\n",
      "0.0029360\n",
      "0.0\n",
      "0.0030453\n",
      "2.0\n",
      "0.0030301\n",
      "4.0\n",
      "0.0029878\n",
      "6.0\n",
      "0.0029249\n",
      "8.0\n",
      "0.0030050\n",
      "10.0\n",
      "0.0029879\n",
      "12.0\n",
      "0.0029540\n",
      "14.0\n",
      "0.0029285\n",
      "16.0\n",
      "0.0029257\n",
      "18.0\n",
      "0.0029411\n",
      "20.0\n",
      "0.0029602\n",
      "0.0\n",
      "0.0030367\n",
      "2.0\n",
      "0.0029566\n",
      "4.0\n",
      "0.0029371\n",
      "6.0\n",
      "0.0029679\n",
      "8.0\n",
      "0.0029701\n",
      "10.0\n",
      "0.0029542\n",
      "12.0\n",
      "0.0029133\n",
      "14.0\n",
      "0.0029689\n",
      "16.0\n",
      "0.0029240\n",
      "18.0\n",
      "0.0029758\n",
      "20.0\n",
      "0.0029044\n",
      "0.0\n",
      "0.0030388\n",
      "2.0\n",
      "0.0030010\n",
      "4.0\n",
      "0.0029518\n",
      "6.0\n",
      "0.0030172\n",
      "8.0\n",
      "0.0029459\n",
      "10.0\n",
      "0.0029454\n",
      "12.0\n",
      "0.0029600\n",
      "14.0\n",
      "0.0029639\n",
      "16.0\n",
      "0.0029548\n",
      "18.0\n",
      "0.0029299\n",
      "20.0\n",
      "0.0029225\n",
      "0.0\n",
      "0.0030233\n",
      "2.0\n",
      "0.0030216\n",
      "4.0\n",
      "0.0029843\n",
      "6.0\n",
      "0.0029461\n",
      "8.0\n",
      "0.0029627\n",
      "10.0\n",
      "0.0029243\n",
      "12.0\n",
      "0.0029329\n",
      "14.0\n",
      "0.0029423\n",
      "16.0\n",
      "0.0029636\n",
      "18.0\n",
      "0.0029671\n",
      "20.0\n",
      "0.0029360\n",
      "0.0\n",
      "0.0030287\n",
      "2.0\n",
      "0.0029993\n",
      "4.0\n",
      "0.0029740\n",
      "6.0\n",
      "0.0029606\n",
      "8.0\n",
      "0.0029700\n",
      "10.0\n",
      "0.0029788\n",
      "12.0\n",
      "0.0029725\n",
      "14.0\n",
      "0.0029362\n",
      "16.0\n",
      "0.0029846\n",
      "18.0\n",
      "0.0029255\n",
      "20.0\n",
      "0.0029317\n",
      "0.0\n",
      "0.0030358\n",
      "2.0\n",
      "0.0029672\n",
      "4.0\n",
      "0.0029532\n",
      "6.0\n",
      "0.0029573\n",
      "8.0\n",
      "0.0030021\n",
      "10.0\n",
      "0.0030333\n",
      "12.0\n",
      "0.0029540\n",
      "14.0\n",
      "0.0029361\n",
      "16.0\n",
      "0.0029551\n",
      "18.0\n",
      "0.0029388\n",
      "20.0\n",
      "0.0029132\n",
      "0.0\n",
      "0.0030223\n",
      "2.0\n",
      "0.0030046\n",
      "4.0\n",
      "0.0029870\n",
      "6.0\n",
      "0.0029661\n",
      "8.0\n",
      "0.0029738\n",
      "10.0\n",
      "0.0029246\n",
      "12.0\n",
      "0.0029348\n",
      "14.0\n",
      "0.0029566\n",
      "16.0\n",
      "0.0029821\n",
      "18.0\n",
      "0.0029514\n",
      "20.0\n",
      "0.0029746\n",
      "0.0\n",
      "0.0030451\n",
      "2.0\n",
      "0.0030002\n",
      "4.0\n",
      "0.0029893\n",
      "6.0\n",
      "0.0030208\n",
      "8.0\n",
      "0.0029248\n",
      "10.0\n",
      "0.0029541\n",
      "12.0\n",
      "0.0029781\n",
      "14.0\n",
      "0.0029444\n",
      "16.0\n",
      "0.0029391\n",
      "18.0\n",
      "0.0028910\n",
      "20.0\n",
      "0.0029788\n",
      "0.0\n",
      "0.0030264\n",
      "2.0\n",
      "0.0030055\n",
      "4.0\n",
      "0.0029756\n",
      "6.0\n",
      "0.0029814\n",
      "8.0\n",
      "0.0029201\n",
      "10.0\n",
      "0.0029446\n",
      "12.0\n",
      "0.0029797\n",
      "14.0\n",
      "0.0029428\n",
      "16.0\n",
      "0.0029407\n",
      "18.0\n",
      "0.0029230\n",
      "20.0\n",
      "0.0029190\n",
      "0.0\n",
      "0.0030593\n",
      "2.0\n",
      "0.0029946\n",
      "4.0\n",
      "0.0029745\n",
      "6.0\n",
      "0.0029420\n",
      "8.0\n",
      "0.0029498\n",
      "10.0\n",
      "0.0029459\n",
      "12.0\n",
      "0.0029921\n",
      "14.0\n",
      "0.0029247\n",
      "16.0\n",
      "0.0029707\n",
      "18.0\n",
      "0.0029359\n",
      "20.0\n",
      "0.0029241\n",
      "0.0\n",
      "0.0030380\n",
      "2.0\n",
      "0.0030210\n",
      "4.0\n",
      "0.0029596\n",
      "6.0\n",
      "0.0029305\n",
      "8.0\n",
      "0.0029713\n",
      "10.0\n",
      "0.0029557\n",
      "12.0\n",
      "0.0029735\n",
      "14.0\n",
      "0.0029674\n",
      "16.0\n",
      "0.0029499\n",
      "18.0\n",
      "0.0029727\n",
      "20.0\n",
      "0.0029327\n",
      "0.0\n",
      "0.0030148\n",
      "2.0\n",
      "0.0030016\n",
      "4.0\n",
      "0.0029779\n",
      "6.0\n",
      "0.0029521\n",
      "8.0\n",
      "0.0029686\n",
      "10.0\n",
      "0.0029384\n",
      "12.0\n",
      "0.0029450\n",
      "14.0\n",
      "0.0029716\n",
      "16.0\n",
      "0.0029772\n",
      "18.0\n",
      "0.0029326\n",
      "20.0\n",
      "0.0029631\n",
      "0.0\n",
      "0.0030141\n",
      "2.0\n",
      "0.0029910\n",
      "4.0\n",
      "0.0029923\n",
      "6.0\n",
      "0.0029996\n",
      "8.0\n",
      "0.0029389\n",
      "10.0\n",
      "0.0029335\n",
      "12.0\n",
      "0.0029447\n",
      "14.0\n",
      "0.0029753\n",
      "16.0\n",
      "0.0029222\n",
      "18.0\n",
      "0.0029504\n",
      "20.0\n",
      "0.0029509\n",
      "0.0\n",
      "0.0030239\n",
      "2.0\n",
      "0.0030093\n",
      "4.0\n",
      "0.0029262\n",
      "6.0\n",
      "0.0029732\n",
      "8.0\n",
      "0.0029691\n",
      "10.0\n",
      "0.0029698\n",
      "12.0\n",
      "0.0029757\n",
      "14.0\n",
      "0.0029281\n",
      "16.0\n",
      "0.0029583\n",
      "18.0\n",
      "0.0029708\n",
      "20.0\n",
      "0.0029251\n",
      "0.0\n",
      "0.0030276\n",
      "2.0\n",
      "0.0029891\n",
      "4.0\n",
      "0.0029597\n",
      "6.0\n",
      "0.0029348\n",
      "8.0\n",
      "0.0029594\n",
      "10.0\n",
      "0.0029507\n",
      "12.0\n",
      "0.0029755\n",
      "14.0\n",
      "0.0029629\n",
      "16.0\n",
      "0.0029548\n",
      "18.0\n",
      "0.0029610\n",
      "20.0\n",
      "0.0029579\n",
      "0.0\n",
      "0.0030366\n",
      "2.0\n",
      "0.0030280\n",
      "4.0\n",
      "0.0030258\n",
      "6.0\n",
      "0.0029576\n",
      "8.0\n",
      "0.0029757\n",
      "10.0\n",
      "0.0029382\n",
      "12.0\n",
      "0.0029705\n",
      "14.0\n",
      "0.0029937\n",
      "16.0\n",
      "0.0029365\n",
      "18.0\n",
      "0.0029389\n",
      "20.0\n",
      "0.0029466\n",
      "0.0\n",
      "0.0030160\n",
      "2.0\n",
      "0.0029990\n",
      "4.0\n",
      "0.0029449\n",
      "6.0\n",
      "0.0029716\n",
      "8.0\n",
      "0.0029627\n",
      "10.0\n",
      "0.0029545\n",
      "12.0\n",
      "0.0029491\n",
      "14.0\n",
      "0.0029635\n",
      "16.0\n",
      "0.0029775\n",
      "18.0\n",
      "0.0029159\n",
      "20.0\n",
      "0.0029984\n",
      "0.0\n",
      "0.0030771\n",
      "2.0\n",
      "0.0029854\n",
      "4.0\n",
      "0.0029561\n",
      "6.0\n",
      "0.0029378\n",
      "8.0\n",
      "0.0029999\n",
      "10.0\n",
      "0.0029349\n",
      "12.0\n",
      "0.0029696\n",
      "14.0\n",
      "0.0029996\n",
      "16.0\n",
      "0.0029690\n",
      "18.0\n",
      "0.0029443\n",
      "20.0\n",
      "0.0029922\n",
      "0.0\n",
      "0.0030633\n",
      "2.0\n",
      "0.0029844\n",
      "4.0\n",
      "0.0029574\n",
      "6.0\n",
      "0.0029607\n",
      "8.0\n",
      "0.0029451\n",
      "10.0\n",
      "0.0029781\n",
      "12.0\n",
      "0.0029641\n",
      "14.0\n",
      "0.0029543\n",
      "16.0\n",
      "0.0029316\n",
      "18.0\n",
      "0.0029569\n",
      "20.0\n",
      "0.0029506\n",
      "0.0\n",
      "0.0030146\n",
      "2.0\n",
      "0.0029792\n",
      "4.0\n",
      "0.0030061\n",
      "6.0\n",
      "0.0029736\n",
      "8.0\n",
      "0.0029772\n",
      "10.0\n",
      "0.0029382\n",
      "12.0\n",
      "0.0029619\n",
      "14.0\n",
      "0.0029493\n",
      "16.0\n",
      "0.0029679\n",
      "18.0\n",
      "0.0029757\n",
      "20.0\n",
      "0.0029542\n",
      "0.0\n",
      "0.0030309\n",
      "2.0\n",
      "0.0029915\n",
      "4.0\n",
      "0.0029902\n",
      "6.0\n",
      "0.0029423\n",
      "8.0\n",
      "0.0029359\n",
      "10.0\n",
      "0.0029376\n",
      "12.0\n",
      "0.0029534\n",
      "14.0\n",
      "0.0029421\n",
      "16.0\n",
      "0.0029493\n",
      "18.0\n",
      "0.0029230\n",
      "20.0\n",
      "0.0029260\n",
      "0.0\n",
      "0.0030682\n",
      "2.0\n",
      "0.0030059\n",
      "4.0\n",
      "0.0029686\n",
      "6.0\n",
      "0.0030108\n",
      "8.0\n",
      "0.0029646\n",
      "10.0\n",
      "0.0029744\n",
      "12.0\n",
      "0.0029398\n",
      "14.0\n",
      "0.0029443\n",
      "16.0\n",
      "0.0029157\n",
      "18.0\n",
      "0.0029114\n",
      "20.0\n",
      "0.0029824\n",
      "0.0\n",
      "0.0030325\n",
      "2.0\n",
      "0.0029605\n",
      "4.0\n",
      "0.0029933\n",
      "6.0\n",
      "0.0029638\n",
      "8.0\n",
      "0.0029602\n",
      "10.0\n",
      "0.0029611\n",
      "12.0\n",
      "0.0029340\n",
      "14.0\n",
      "0.0029325\n",
      "16.0\n",
      "0.0029597\n",
      "18.0\n",
      "0.0029129\n",
      "20.0\n",
      "0.0029687\n",
      "0.0\n",
      "0.0030074\n",
      "2.0\n",
      "0.0029891\n",
      "4.0\n",
      "0.0029771\n",
      "6.0\n",
      "0.0029764\n",
      "8.0\n",
      "0.0029511\n",
      "10.0\n",
      "0.0029557\n",
      "12.0\n",
      "0.0029562\n",
      "14.0\n",
      "0.0029440\n",
      "16.0\n",
      "0.0029933\n",
      "18.0\n",
      "0.0029226\n",
      "20.0\n",
      "0.0029492\n",
      "0.0\n",
      "0.0030418\n",
      "2.0\n",
      "0.0029919\n",
      "4.0\n",
      "0.0029795\n",
      "6.0\n",
      "0.0029565\n",
      "8.0\n",
      "0.0029577\n",
      "10.0\n",
      "0.0029546\n",
      "12.0\n",
      "0.0029646\n",
      "14.0\n",
      "0.0029412\n",
      "16.0\n",
      "0.0029252\n",
      "18.0\n",
      "0.0029495\n",
      "20.0\n",
      "0.0029445\n",
      "0.0\n",
      "0.0030432\n",
      "2.0\n",
      "0.0030199\n",
      "4.0\n",
      "0.0029591\n",
      "6.0\n",
      "0.0029727\n",
      "8.0\n",
      "0.0029572\n",
      "10.0\n",
      "0.0029275\n",
      "12.0\n",
      "0.0029596\n",
      "14.0\n",
      "0.0029520\n",
      "16.0\n",
      "0.0029638\n",
      "18.0\n",
      "0.0029313\n",
      "20.0\n",
      "0.0029798\n",
      "0.0\n",
      "0.0030585\n",
      "2.0\n",
      "0.0030133\n",
      "4.0\n",
      "0.0029671\n",
      "6.0\n",
      "0.0029493\n",
      "8.0\n",
      "0.0030106\n",
      "10.0\n",
      "0.0029376\n",
      "12.0\n",
      "0.0029377\n",
      "14.0\n",
      "0.0029545\n",
      "16.0\n",
      "0.0029401\n",
      "18.0\n",
      "0.0029862\n",
      "20.0\n",
      "0.0029230\n"
     ]
    }
   ],
   "source": [
    "SNR_min_dB  = 0\n",
    "SNR_max_dB  = 20\n",
    "step_dB     = 2\n",
    "num_dB      = int((SNR_max_dB - SNR_min_dB) / step_dB) + 1\n",
    "\n",
    "SNR         = np.linspace(SNR_min_dB, SNR_max_dB, num=num_dB)\n",
    "log         = './model/log_test.txt'\n",
    "\n",
    "N_test = int(N_samp)\n",
    "\n",
    "for i in range (100):\n",
    "    for snr in SNR:\n",
    "        with Record(log):\n",
    "            print(snr)\n",
    "        DataSet_x, DataSet_y, DataSet_H, DataSet_HH = Gen_dataset('test', snr, 0, N_test)\n",
    "        H_true, H_raw, H_in, e, xTx, xTy, Di = Input_ISDNN('test', DataSet_x, DataSet_y, DataSet_H, DataSet_HH, N_test)\n",
    "        \n",
    "        test(H_raw, Di, H_in, e, xTx, xTy, N_test, log)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b540dc7c34b7cf08e01b749cd784ab7b1f302abefb3330aee8479ab39c2f7339"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
