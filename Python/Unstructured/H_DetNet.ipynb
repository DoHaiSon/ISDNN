{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSmcwLZuptDU"
   },
   "source": [
    "# Proposed Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG5fZn_ypunP"
   },
   "source": [
    "# **Biblioheque**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "--SvzvIspu-U"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from scipy.stats import rice\n",
    "import pickle\n",
    "# import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import sys\n",
    "import timeit\n",
    "import os\n",
    "\n",
    "# torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBk01Vu0pvMm"
   },
   "source": [
    "# class to save results in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "06IymVCkpvYS"
   },
   "outputs": [],
   "source": [
    "class Record:\n",
    "    def __init__(self, TextName):\n",
    "        self.out_file = open(TextName, 'a')\n",
    "        self.old_stdout = sys.stdout\n",
    "        sys.stdout = self\n",
    "\n",
    "    def write(self, text):\n",
    "        self.old_stdout.write(text)\n",
    "        self.out_file.write(text)\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout = self.old_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xPamGjNpv8E"
   },
   "source": [
    "# **slicer the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dlQdA0xHpwF7"
   },
   "outputs": [],
   "source": [
    "def slicer(data):\n",
    "    dataI = data[slice(0, len(data), 2)]\n",
    "    dataQ = data[slice(1, len(data), 2)]\n",
    "    return(dataI, dataQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOoiIo_LpwQl"
   },
   "source": [
    "# **Modulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xIftC8fvpwir"
   },
   "outputs": [],
   "source": [
    "def mapper_16QAM(QAM16, data):\n",
    "    map0 = 2*data[slice(0, len(data), 2)] + data[slice(1, len(data), 2)]\n",
    "    map0 = list(map(int, map0))\n",
    "    dataMapped = []\n",
    "    for i in range(len(map0)):\n",
    "        dataMapped.append(QAM16[map0[i]])\n",
    "    return(dataMapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AYhPgAIcqtGT"
   },
   "outputs": [],
   "source": [
    "def calculate_bits(Modulation,NumSubcarriers,NumDataSymb):\n",
    "    if Modulation=='QPSK':\n",
    "        Nbpscs=2\n",
    "    elif Modulation=='16QAM':\n",
    "        Nbpscs=4\n",
    "    return NumDataSymb*NumSubcarriers*Nbpscs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgX6tiqFpwvb"
   },
   "source": [
    "# **generate noise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2LQwZvGaprKZ"
   },
   "outputs": [],
   "source": [
    "def AWGN(IFsig, SNR):\n",
    "    dP = np.zeros(len(IFsig))\n",
    "    P = 0\n",
    "\n",
    "    for i in range(len(IFsig)):\n",
    "        dP[i] = abs(IFsig[i])**2\n",
    "        P = P + dP[i]\n",
    "\n",
    "    P = P/len(IFsig)\n",
    "    gamma = 10**(SNR/10)\n",
    "    N0 = P/gamma\n",
    "    n = ((N0/2)**(0.5))*np.random.standard_normal(len(IFsig))\n",
    "    IF_n = np.zeros((len(IFsig),1))\n",
    "\n",
    "    for i in range(len(IFsig)):\n",
    "        IF_n[i,:] = IFsig[i] + n[i]\n",
    "\n",
    "    return(IF_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate channel model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_channel(Nr, Nt, type):\n",
    "    if (type == 'gauss'):\n",
    "        return (np.random.normal(size=(Nr,Nt))+1j*np.random.normal(size=(Nr,Nt)))/np.sqrt(2)\n",
    "    if (type == 'rayleigh'):\n",
    "        return (np.random.rayleigh(scale=(1/np.sqrt(2)), size=(Nr,Nt)) + 1j*np.random.rayleigh(scale=(1/np.sqrt(2)), size=(Nr,Nt)))/np.sqrt(2)\n",
    "    if (type == 'rician'):\n",
    "        b = 1/np.sqrt(2)\n",
    "        return (rice.rvs(b, size=(Nr,Nt)) + 1j*rice.rvs(b, size=(Nr,Nt)))/np.sqrt(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eciNtnFjq2yd"
   },
   "source": [
    "# **Generate Dataset**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AE4Q6jZCq3CY"
   },
   "outputs": [],
   "source": [
    "DataSet_x   = []  # x dataset after modulation\n",
    "DataSet_y   = []  # y dataset\n",
    "DataSet_HH  = []  # H dataset\n",
    "DataSet_b   = []  # binary dataset\n",
    "SNR_min_dB  = 0\n",
    "SNR_max_dB  = 20\n",
    "step_dB     = 5\n",
    "num_dB      = int((SNR_max_dB - SNR_min_dB) / step_dB) + 1\n",
    "\n",
    "SNR         = np.linspace(SNR_min_dB, SNR_max_dB, num=num_dB)\n",
    "\n",
    "\n",
    "Nt = 2             # Tx: 8\n",
    "Nr = 72            # Rx: 128\n",
    "N_samp = 10000\n",
    "\n",
    "\n",
    "def Gen_dataset(mode, snr, imperfect, N_samp):    \n",
    "    DataSet_x   = []  # x dataset after modulation\n",
    "    DataSet_y   = []  # y dataset\n",
    "    DataSet_H   = []  \n",
    "    DataSet_HH  = []\n",
    "\n",
    "    NumSubcarriers = 1\n",
    "    Modulation = '16QAM'\n",
    "    QAM16 = [-1, -0.333, 0.333, 1]\n",
    "    NumDataSymb = 1\n",
    "    N_type = 'gauss'\n",
    "\n",
    "    if mode == 'train':\n",
    "        for snr in SNR:\n",
    "            for runIdx in range(0, N_samp):      # ! 20000 x Nt: samples\n",
    "                H = Generate_channel(Nt, Nr, N_type)\n",
    "                HH = np.concatenate((np.concatenate((H.real, H.imag), axis=1),\n",
    "                                    np.concatenate((-H.imag, H.real), axis=1)), axis=0)\n",
    "                x = np.zeros((2*Nt, NumSubcarriers))\n",
    "                a = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
    "                DataRaw = np.zeros((Nt, a))\n",
    "                for t in range(Nt):\n",
    "                    #\"data symbol generate\"\n",
    "                    NumBits = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
    "                    bit = np.random.randint(1, 3, NumBits)-1\n",
    "                    DataRaw[t, :] = bit\n",
    "                    for j in range(4):\n",
    "                        DataSet_b.append(bit[j])\n",
    "                    I = np.zeros((1, a))\n",
    "                    I[0, :] = DataRaw[t, :]\n",
    "                    (dataI, dataQ) = slicer(I[0])\n",
    "\n",
    "                    # Mapper\n",
    "                    mapI = mapper_16QAM(QAM16, dataI)\n",
    "                    mapQ = mapper_16QAM(QAM16, dataQ)\n",
    "                    x[t] = mapI[0]\n",
    "                    x[t+Nt] = mapQ[0]\n",
    "\n",
    "                # transpose\n",
    "                x = x.transpose()\n",
    "\n",
    "                y_wo_noise = np.matmul(x, HH)\n",
    "\n",
    "                # noise\n",
    "                noise = AWGN(y_wo_noise.transpose(), snr)\n",
    "\n",
    "                y = y_wo_noise + noise.transpose()\n",
    "\n",
    "                DataSet_x.append(x)    # ! I, Q sample distance by Nt.\n",
    "                DataSet_y.append(y)                 # ! output sample\n",
    "                \n",
    "                # Imperfect channel: 5%\n",
    "                # coef = (2*np.random.randint(0,2,size=HH.shape) - 1)\n",
    "                # HH = HH + coef * HH * 0.05\n",
    "                DataSet_HH.append(HH)\n",
    "                DataSet_H.append(H)               # ! Generated channel\n",
    "                \n",
    "    else:\n",
    "        for runIdx in range(0, N_samp):      # ! 20000 x Nt: samples\n",
    "            H = Generate_channel(Nt, Nr, N_type)\n",
    "            HH = np.concatenate((np.concatenate((H.real, H.imag), axis=1),\n",
    "                                np.concatenate((-H.imag, H.real), axis=1)), axis=0)\n",
    "            x = np.zeros((2*Nt, NumSubcarriers))\n",
    "            a = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
    "            DataRaw = np.zeros((Nt, a))\n",
    "            for t in range(Nt):\n",
    "                #\"data symbol generate\"\n",
    "                NumBits = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
    "                bit = np.random.randint(1, 3, NumBits)-1\n",
    "                DataRaw[t, :] = bit\n",
    "                for j in range(4):\n",
    "                    DataSet_b.append(bit[j])\n",
    "                I = np.zeros((1, a))\n",
    "                I[0, :] = DataRaw[t, :]\n",
    "                (dataI, dataQ) = slicer(I[0])\n",
    "\n",
    "                # Mapper\n",
    "                mapI = mapper_16QAM(QAM16, dataI)\n",
    "                mapQ = mapper_16QAM(QAM16, dataQ)\n",
    "                x[t] = mapI[0]\n",
    "                x[t+Nt] = mapQ[0]\n",
    "\n",
    "            # transpose\n",
    "            x = x.transpose()\n",
    "\n",
    "            y_wo_noise = np.matmul(x, HH)\n",
    "\n",
    "            # noise\n",
    "            noise = AWGN(y_wo_noise.transpose(), snr)\n",
    "\n",
    "            y = y_wo_noise + noise.transpose()\n",
    "\n",
    "            DataSet_x.append(x)    # ! I, Q sample distance by Nt.\n",
    "            DataSet_y.append(y)                 # ! output sample\n",
    "            \n",
    "            # Imperfect channel: 5%\n",
    "            DataSet_HH.append(HH)\n",
    "            DataSet_H.append(H)               # ! Generated channel\n",
    "\n",
    "\n",
    "    # Shuffle dataset\n",
    "    random.seed(1)\n",
    "    temp = list(zip(DataSet_x, DataSet_y, DataSet_H, DataSet_HH))\n",
    "    random.shuffle(temp)\n",
    "    DataSet_x, DataSet_y, DataSet_H, DataSet_HH = zip(*temp)\n",
    "\n",
    "    return DataSet_x, DataSet_y, DataSet_H, DataSet_HH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_channel (H):\n",
    "# H_raw = [R(H) I(H); -I(H) R(H)]\n",
    "# we have four version of H_est\n",
    "    H_est_1 = []\n",
    "    H_est_2 = []\n",
    "    H_est_3 = []\n",
    "    H_est_4 = []\n",
    "\n",
    "    H_est_Re_1 = H[0:Nt, 0:Nr]\n",
    "    H_est_Im_1 = H[0:Nt, Nr:2*Nr]\n",
    "    H_est_Im_2 = - H[Nt:2*Nt, 0:Nr]\n",
    "    H_est_Re_2 = H[Nt:2*Nt, Nr:2*Nr]\n",
    "\n",
    "    H_est_1 = H_est_Re_1 + 1j * H_est_Im_1\n",
    "    H_est_2 = H_est_Re_1 + 1j * H_est_Im_2\n",
    "    H_est_3 = H_est_Re_2 + 1j * H_est_Im_1\n",
    "    H_est_4 = H_est_Re_2 + 1j * H_est_Im_2\n",
    "    \n",
    "    return H_est_1, H_est_2, H_est_3, H_est_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def NMSE(H_est, H_raw):\n",
    "#     H_est_1, H_est_2, H_est_3, H_est_4 = reconstruct_channel(H_est)\n",
    "#     H_est_vec_1 = torch.reshape(H_est_1, [Nt * Nr, 1])\n",
    "#     H_est_vec_2 = torch.reshape(H_est_2, [Nt * Nr, 1])\n",
    "#     H_est_vec_3 = torch.reshape(H_est_3, [Nt * Nr, 1])\n",
    "#     H_est_vec_4 = torch.reshape(H_est_4, [Nt * Nr, 1])\n",
    "\n",
    "#     H_raw_vec = torch.reshape(H_raw, [Nt * Nr, 1])\n",
    "\n",
    "#     mse_1       = (torch.norm(H_raw_vec - H_est_vec_1)**2) / len(H_raw_vec)\n",
    "#     mse_2       = (torch.norm(H_raw_vec - H_est_vec_2)**2) / len(H_raw_vec)\n",
    "#     mse_3       = (torch.norm(H_raw_vec - H_est_vec_3)**2) / len(H_raw_vec)\n",
    "#     mse_4       = (torch.norm(H_raw_vec - H_est_vec_4)**2) / len(H_raw_vec)\n",
    "\n",
    "#     sigEner   = torch.norm(H_raw_vec)**2\n",
    "\n",
    "#     nmse_1      = mse_1 / sigEner\n",
    "#     nmse_2      = mse_2 / sigEner\n",
    "#     nmse_3      = mse_3 / sigEner\n",
    "#     nmse_4      = mse_4 / sigEner\n",
    "\n",
    "#     # Best nmse\n",
    "#     nmse        = min([nmse_1, nmse_2, nmse_3, nmse_4])\n",
    "\n",
    "#     return torch.abs(nmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMSE(H_est, H_raw):\n",
    "    H_est_1, H_est_2, H_est_3, H_est_4 = reconstruct_channel(H_est)\n",
    "    \n",
    "    # Lấy phần thực của các tensor nếu chúng là complex\n",
    "    H_est_vec_1 = torch.reshape(H_est_1, [Nt * Nr, 1]).abs()\n",
    "    H_est_vec_2 = torch.reshape(H_est_2, [Nt * Nr, 1]).abs()\n",
    "    H_est_vec_3 = torch.reshape(H_est_3, [Nt * Nr, 1]).abs()\n",
    "    H_est_vec_4 = torch.reshape(H_est_4, [Nt * Nr, 1]).abs()\n",
    "\n",
    "    H_raw_vec = torch.reshape(H_raw, [Nt * Nr, 1]).abs()\n",
    "\n",
    "    mse_1 = (torch.norm(H_raw_vec - H_est_vec_1)**2) / len(H_raw_vec)\n",
    "    mse_2 = (torch.norm(H_raw_vec - H_est_vec_2)**2) / len(H_raw_vec)\n",
    "    mse_3 = (torch.norm(H_raw_vec - H_est_vec_3)**2) / len(H_raw_vec)\n",
    "    mse_4 = (torch.norm(H_raw_vec - H_est_vec_4)**2) / len(H_raw_vec)\n",
    "\n",
    "    sigEner = torch.norm(H_raw_vec)**2\n",
    "\n",
    "    nmse_1 = mse_1 / sigEner\n",
    "    nmse_2 = mse_2 / sigEner\n",
    "    nmse_3 = mse_3 / sigEner\n",
    "    nmse_4 = mse_4 / sigEner\n",
    "\n",
    "    # Chọn NMSE tốt nhất\n",
    "    nmse = min([nmse_1, nmse_2, nmse_3, nmse_4])\n",
    "\n",
    "    return torch.abs(nmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pyjIgxUurU0P"
   },
   "outputs": [],
   "source": [
    "def Input_ISDNN(mode, DataSet_x, DataSet_y, DataSet_H, DataSet_HH, N_samp):\n",
    "    H_in = []        # ! H_in    , np.diag(np.diag()) return a diag matrix instead of diag components.\n",
    "    H_true = []   # ! generated s\n",
    "    H_raw = []\n",
    "    v = []        # ! vector errors\n",
    "    xTx = []\n",
    "    xTy = []\n",
    "    # steering = [] # ! Steering vector: ZoA and AoA\n",
    "\n",
    "    if mode == 'train':\n",
    "        n_sample = N_samp * len(SNR)\n",
    "    else:\n",
    "        n_sample = N_samp\n",
    "        \n",
    "    for i in range (n_sample):\n",
    "        H_true.append(torch.tensor(DataSet_HH[i]))\n",
    "        H_raw.append(torch.tensor(DataSet_H[i]))\n",
    "        xTy.append(torch.tensor(np.dot(DataSet_x[i].transpose(), DataSet_y[i])))\n",
    "        H_in.append(torch.zeros([2*Nt, 2*Nr]))   \n",
    "        v.append(torch.zeros([2*Nt, 2*Nr]))\n",
    "        xTx.append(torch.tensor(np.dot(DataSet_x[i].transpose(), DataSet_x[i])))\n",
    "        # steering.append(torch.tensor(DataSet_Steering[i]))\n",
    "\n",
    "    H_true = torch.stack(H_true, dim=0)\n",
    "    H_raw = torch.stack(H_raw, dim=0)\n",
    "    H_in = torch.stack(H_in, dim=0)\n",
    "    v = torch.stack(v, dim=0)\n",
    "    xTx = torch.stack(xTx, dim=0)\n",
    "    xTy = torch.stack(xTy, dim=0)\n",
    "    # steering = torch.stack(steering, dim=0)\n",
    "\n",
    "    return H_true, H_raw, H_in, v, xTx, xTy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGhdBsghq3M9"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "GHceh5kuq3ZD"
   },
   "outputs": [],
   "source": [
    "class xv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(xv, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(4*Nr, 2*Nr)\n",
    "        self.fc2 = torch.nn.Linear(2*Nr, 2*Nr)\n",
    "        self.fc3 = torch.nn.Linear(2*Nr, 2*Nr)\n",
    "        \n",
    "        self.delta_1 = torch.nn.parameter.Parameter(torch.rand(1))\n",
    "        self.delta_2 = torch.nn.parameter.Parameter(torch.rand(1))\n",
    "\n",
    "    def forward(self, H, v, xTx, xTy):\n",
    "\n",
    "        xTxH = torch.matmul(xTx, H)\n",
    "\n",
    "        q    = H - self.delta_1 * xTy + self.delta_2 * xTxH\n",
    "\n",
    "        concat = torch.concat([q, v], 1)\n",
    "\n",
    "        z    = torch.tanh(self.fc1(concat))\n",
    "\n",
    "        H_oh = self.fc2(z)\n",
    "\n",
    "        v    = self.fc3(z)\n",
    "\n",
    "        return H_oh, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "woRjD7lJssRq"
   },
   "outputs": [],
   "source": [
    "class model_driven(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model_driven, self).__init__()\n",
    "\n",
    "        self.layer1=xv()\n",
    "        self.layer2=xv()\n",
    "        self.layer3=xv()\n",
    "        self.layer4=xv()\n",
    "        self.layer5=xv()\n",
    "    \n",
    "    def forward(self, H_in, v, xTx, xTy):\n",
    "        H_oh, v = self.layer1(H_in, v, xTx, xTy)\n",
    "        H       = torch.tanh(H_oh)\n",
    "\n",
    "        H_oh, v = self.layer2(H_in, v, xTx, xTy)\n",
    "        H       = torch.tanh(H_oh)\n",
    "\n",
    "        H_oh, v = self.layer3(H_in, v, xTx, xTy)\n",
    "        H       = torch.tanh(H_oh)\n",
    "\n",
    "        H_oh, v = self.layer4(H_in, v, xTx, xTy)\n",
    "\n",
    "        return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZjihTOXq3kG"
   },
   "source": [
    "# Define model, optimizer, and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Vp9fRd3gq3tw"
   },
   "outputs": [],
   "source": [
    "def def_model():\n",
    "    model = model_driven()\n",
    "    loss = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    folder_model = './model/'\n",
    "    \n",
    "    if not os.path.isdir(folder_model):\n",
    "        os.makedirs(folder_model)\n",
    "    \n",
    "    file_model = folder_model + 'H'\n",
    "    # if os.path.isfile(file_model):\n",
    "    #     generator = torch.load(file_model)\n",
    "\n",
    "    record_file = 'H'\n",
    "    return model, loss, optimizer, record_file, file_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYWM7SzItKzS"
   },
   "source": [
    "# Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jv7lDwyxtFe3"
   },
   "outputs": [],
   "source": [
    "epoch         = 0\n",
    "expected_epoch = 20000\n",
    "num_samp      = N_samp * len(SNR)\n",
    "best_nmse     = 1e9\n",
    "early_stop    = 0\n",
    "best_model    = ''\n",
    "batch_size    = 1\n",
    "# Kiểm tra nếu file tĩnh tồn tại\n",
    "if os.path.exists('dataset_DetNet.pkl'):\n",
    "    # Nếu tồn tại, tải dữ liệu từ file tĩnh\n",
    "    with open('dataset_DetNet.pkl', 'rb') as f:\n",
    "        DataSet_x, DataSet_y, DataSet_H, DataSet_HH, H_true, H_raw, H_in, v, xTx, xTy = pickle.load(f)\n",
    "    print(\"Dữ liệu đã được tải từ file tĩnh!\")\n",
    "else:\n",
    "    # Sinh dữ liệu nếu file tĩnh không tồn tại\n",
    "    DataSet_x, DataSet_y, DataSet_H, DataSet_HH = Gen_dataset('train', 0, 0, N_samp)\n",
    "    H_true, H_raw, H_in, v, xTx, xTy = Input_ISDNN('train', DataSet_x, DataSet_y, DataSet_H, DataSet_HH, N_samp)\n",
    "    \n",
    "    # Lưu dữ liệu để lần sau không phải sinh lại\n",
    "    with open('dataset_DetNet.pkl', 'wb') as f:\n",
    "        pickle.dump((DataSet_x, DataSet_y, DataSet_H, DataSet_HH, H_true, H_raw, H_in, v, xTx, xTy), f)\n",
    "    print(\"Dữ liệu đã được sinh và lưu lại!\")\n",
    "\n",
    "print(\"Begin training...\") \n",
    "starttime = timeit.default_timer()\n",
    "\n",
    "while(True):\n",
    "        epoch = epoch + 1 \n",
    "\n",
    "        init_loss = 1e9\n",
    "        while( epoch == 1 and init_loss > 260):\n",
    "                \n",
    "                model, loss, optimizer, record_file, file_model = def_model()\n",
    "                init_loss = 0\n",
    "                for bs in range (int(num_samp / batch_size)):\n",
    "                    H_1 = model(\n",
    "                                 torch.squeeze(H_in[0 + batch_size * bs:batch_size * (bs+1), :, :]), \n",
    "                                 torch.squeeze(v[0 + batch_size * bs:batch_size * (bs+1), :, :]), \n",
    "                                 torch.squeeze(xTx[0 + batch_size * bs:batch_size * (bs+1), :, :]), \n",
    "                                 torch.squeeze(xTy[0 + batch_size * bs:batch_size * (bs+1), :, :]))   # predict output from the model\n",
    "                    init_loss += loss(H_1, torch.squeeze(H_true[0 + batch_size * bs:batch_size * (bs+1), :, :])).item()\n",
    "                print(init_loss)\n",
    "\n",
    "        optimizer.zero_grad()   # zero the parameter gradients\n",
    "        train_loss = 0\n",
    "        H_f = torch.empty([num_samp, 2*Nt, 2*Nr])\n",
    "        for bs in range (int(num_samp / batch_size)):\n",
    "                H_o = model(\n",
    "                        torch.squeeze(H_in[0 + batch_size * bs:batch_size * (bs+1), :, :]), \n",
    "                        torch.squeeze(v[0 + batch_size * bs:batch_size * (bs+1), :, :]), \n",
    "                        torch.squeeze(xTx[0 + batch_size * bs:batch_size * (bs+1), :, :]), \n",
    "                        torch.squeeze(xTy[0 + batch_size * bs:batch_size * (bs+1), :, :]))   # predict output from the model\n",
    "                H_f[0 + batch_size * bs:batch_size * (bs+1), :, :] = H_o\n",
    "                train_loss = loss(H_o, \n",
    "                                  torch.squeeze(H_true[0 + batch_size * bs:batch_size * (bs+1), :, :]))   # calculate loss for the predicted output  \n",
    "                train_loss.backward()   # backpropagate the loss \n",
    "                optimizer.step()        # adjust parameters based on the calculated gradients \n",
    "\n",
    "        if (epoch % 100 == 0 or epoch == 1):\n",
    "                nmse = 0\n",
    "                for j in range (num_samp):\n",
    "                        nmse += NMSE(H_f[j], H_raw[j])\n",
    "                nmse = nmse / num_samp\n",
    "                \n",
    "                if (nmse <= best_nmse):\n",
    "                        torch.save(model.state_dict(), file_model + '_' + str(epoch) + '.pth')\n",
    "                        best_model = file_model + '_' + str(epoch) + '.pth'\n",
    "                        best_nmse = nmse\n",
    "                        early_stop = 0\n",
    "                else:\n",
    "                        early_stop += 1\n",
    "\n",
    "                if (nmse > best_nmse and early_stop == 3):\n",
    "                        with Record(record_file + '_log.txt'):\n",
    "                                print(epoch, nmse.item(), train_loss.item()) \n",
    "                                print(str(timeit.default_timer()-starttime))\n",
    "                        break\n",
    "\n",
    "                with Record(record_file + '_log.txt'):\n",
    "                        print(epoch, nmse.item(), train_loss.item()) \n",
    "\n",
    "        if epoch  == expected_epoch:\n",
    "                torch.save(model.state_dict(), file_model + '_' + str(epoch) + '.pth')\n",
    "                best_model = file_model + '_' + str(epoch) + '.pth'\n",
    "                with Record(record_file + '_log.txt'):\n",
    "                        print(\"epoch:\\n\", epoch)\n",
    "                        print(\"Latest NMSE:\\n\", nmse.item())\n",
    "                        print(\"Latest Loss:\\n\", train_loss.item()) \n",
    "                        print(str(timeit.default_timer()-starttime))\n",
    "\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOBK-_l-tRMO"
   },
   "source": [
    "# Test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRRFGAX4tg_6"
   },
   "source": [
    "# Function to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model=r'C:\\Users\\SON\\Desktop\\ISDNN\\Python\\Unstructured\\model_detnet_10k_4l_1\\H_1.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "SfvpexfwthNq"
   },
   "outputs": [],
   "source": [
    "# from scipy.io import savemat\n",
    "\n",
    "def test(H_raw, H_in, v, xTx, xTy, N_test, log): \n",
    "    # Load the model that we saved at the end of the training loop \n",
    "    model = model_driven()\n",
    "    model.load_state_dict(torch.load(best_model, map_location=torch.device('cpu'))) \n",
    "      \n",
    "    with torch.no_grad(): \n",
    "        H_f = torch.empty([N_test, 2*Nt, 2*Nr])\n",
    "        for bs in range (int(N_test / 1)):\n",
    "            H_o = model(\n",
    "                        torch.squeeze(H_in[0 + batch_size * bs:batch_size * (bs+1), :, :]), \n",
    "                        torch.squeeze(v[0 + batch_size * bs:batch_size * (bs+1), :, :]), \n",
    "                        torch.squeeze(xTx[0 + batch_size * bs:batch_size * (bs+1), :, :]), \n",
    "                        torch.squeeze(xTy[0 + batch_size * bs:batch_size * (bs+1), :, :]))   # predict output from the model\n",
    "            H_f[0 + batch_size * bs:batch_size * (bs+1), :, :] = H_o\n",
    "\n",
    "        nmse = 0\n",
    "        for j in range (N_test):\n",
    "            nmse += NMSE(H_f[j], H_raw[j])\n",
    "\n",
    "        nmse = nmse / N_test\n",
    "        with Record(log):\n",
    "            print(format(nmse.item(), '.7f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate dataset for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LS(DataSet_x, DataSet_y):\n",
    "    start = timeit.default_timer()\n",
    "    for i in range (len(DataSet_x)):\n",
    "        H_hat = np.matmul(\n",
    "                    np.matmul(\n",
    "                        np.linalg.pinv(np.matmul(DataSet_x[i].transpose(), DataSet_x[i])),\n",
    "                        DataSet_x[i].transpose()),\n",
    "                        DataSet_y[i])\n",
    "    print(timeit.default_timer() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MMSE(DataSet_x, DataSet_y, noise_dB, H_raw):\n",
    "    snr_dB = 10 ** (-noise_dB / 10)\n",
    "    start = timeit.default_timer()\n",
    "    # H_f = np.empty([len(DataSet_x), 2*Nt, 2*Nr])\n",
    "    for i in range (len(DataSet_x)):\n",
    "        H_hat = np.matmul(\n",
    "                    np.matmul(\n",
    "                        np.linalg.pinv(np.matmul(DataSet_x[i].transpose(), DataSet_x[i]) + snr_dB * np.eye(2*Nt, 2*Nt)),\n",
    "                        DataSet_x[i].transpose()),\n",
    "                        DataSet_y[i])  \n",
    "        # H_f[i, :, :] = H_hat\n",
    "    print(timeit.default_timer() - start)\n",
    "    # nmse = 0\n",
    "    # for j in range (len(DataSet_x)):\n",
    "    #         # tmp =  H_o[j]\n",
    "    #         # tmp1 = tmp.numpy()\n",
    "    #         # savemat('H_est.mat', {'H_o': tmp1})\n",
    "    #     nmse += NMSE(torch.tensor(H_f[j]), torch.tensor(H_raw[j]))\n",
    "    # nmse = nmse / len(DataSet_x)\n",
    "    # print(format(nmse.item(), '.7f'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0033649\n",
      "0.0032983\n",
      "0.0033897\n",
      "0.0033418\n",
      "0.0033473\n",
      "0.0034322\n",
      "0.0033836\n",
      "0.0033189\n",
      "0.0033261\n",
      "0.0034232\n",
      "0.0034502\n",
      "0.0033723\n",
      "0.0034096\n",
      "0.0033356\n",
      "0.0033877\n",
      "0.0033375\n",
      "0.0033464\n",
      "0.0034655\n",
      "0.0033067\n",
      "0.0033477\n",
      "0.0033777\n",
      "0.0033407\n",
      "0.0033861\n",
      "0.0034001\n",
      "0.0034231\n",
      "0.0034218\n",
      "0.0033979\n",
      "0.0034336\n",
      "0.0034540\n",
      "0.0033539\n",
      "0.0033837\n",
      "0.0034576\n",
      "0.0033279\n",
      "0.0034617\n",
      "0.0033911\n",
      "0.0034441\n",
      "0.0033317\n",
      "0.0034068\n",
      "0.0033947\n",
      "0.0033417\n",
      "0.0033832\n",
      "0.0033173\n",
      "0.0033446\n",
      "0.0033284\n",
      "0.0033976\n",
      "0.0033540\n",
      "0.0033709\n",
      "0.0033193\n",
      "0.0033696\n",
      "0.0033752\n",
      "0.0034055\n",
      "0.0033119\n",
      "0.0033819\n",
      "0.0034480\n",
      "0.0034594\n",
      "0.0034958\n",
      "0.0033646\n",
      "0.0033817\n",
      "0.0033379\n",
      "0.0034625\n",
      "0.0033576\n",
      "0.0034007\n",
      "0.0033539\n",
      "0.0033593\n",
      "0.0033269\n",
      "0.0033768\n",
      "0.0033632\n",
      "0.0033302\n",
      "0.0034059\n",
      "0.0033375\n",
      "0.0034342\n",
      "0.0034266\n",
      "0.0033292\n",
      "0.0033866\n",
      "0.0034363\n",
      "0.0033443\n",
      "0.0034352\n",
      "0.0034260\n",
      "0.0034943\n",
      "0.0032846\n",
      "0.0033706\n",
      "0.0033005\n",
      "0.0033488\n",
      "0.0033870\n",
      "0.0033723\n",
      "0.0034639\n",
      "0.0034570\n",
      "0.0034167\n",
      "0.0033497\n",
      "0.0033735\n",
      "0.0033600\n",
      "0.0033740\n",
      "0.0034887\n",
      "0.0034658\n",
      "0.0033644\n",
      "0.0033140\n",
      "0.0033646\n",
      "0.0033033\n",
      "0.0034685\n",
      "0.0033460\n",
      "0.0033869\n",
      "0.0033280\n",
      "0.0034246\n",
      "0.0033368\n",
      "0.0033709\n",
      "0.0033263\n",
      "0.0032757\n",
      "0.0034353\n",
      "0.0033316\n",
      "0.0033825\n",
      "0.0033793\n",
      "0.0033720\n",
      "0.0034122\n",
      "0.0033196\n",
      "0.0033455\n",
      "0.0034287\n",
      "0.0032934\n",
      "0.0033834\n",
      "0.0034075\n",
      "0.0033874\n",
      "0.0033416\n",
      "0.0034345\n",
      "0.0033694\n",
      "0.0034264\n",
      "0.0034127\n",
      "0.0033344\n",
      "0.0033708\n",
      "0.0033880\n",
      "0.0033960\n",
      "0.0033584\n",
      "0.0033876\n",
      "0.0033888\n",
      "0.0033494\n",
      "0.0033934\n",
      "0.0034409\n",
      "0.0034740\n",
      "0.0034218\n",
      "0.0034423\n",
      "0.0033714\n",
      "0.0033389\n",
      "0.0033911\n",
      "0.0033324\n",
      "0.0033325\n",
      "0.0034110\n",
      "0.0033967\n",
      "0.0033844\n",
      "0.0032968\n",
      "0.0034000\n",
      "0.0033860\n",
      "0.0033416\n",
      "0.0035129\n",
      "0.0033365\n",
      "0.0033475\n",
      "0.0033421\n",
      "0.0033494\n",
      "0.0033138\n",
      "0.0033987\n",
      "0.0034237\n",
      "0.0033644\n",
      "0.0032554\n",
      "0.0034019\n",
      "0.0034155\n",
      "0.0034281\n",
      "0.0033868\n",
      "0.0034331\n",
      "0.0033930\n",
      "0.0033796\n",
      "0.0033895\n",
      "0.0033329\n",
      "0.0032838\n",
      "0.0033746\n",
      "0.0033301\n",
      "0.0033680\n",
      "0.0032713\n",
      "0.0033241\n",
      "0.0034123\n",
      "0.0033962\n",
      "0.0033723\n",
      "0.0033202\n",
      "0.0034296\n",
      "0.0033907\n",
      "0.0033506\n",
      "0.0033568\n",
      "0.0033856\n",
      "0.0033399\n",
      "0.0033613\n",
      "0.0033530\n",
      "0.0034330\n",
      "0.0033521\n",
      "0.0034247\n",
      "0.0033510\n",
      "0.0033699\n",
      "0.0033479\n",
      "0.0033196\n",
      "0.0034025\n",
      "0.0034030\n",
      "0.0033875\n",
      "0.0034559\n",
      "0.0032895\n",
      "0.0034226\n",
      "0.0033729\n",
      "0.0033974\n",
      "0.0034333\n",
      "0.0033835\n",
      "0.0033736\n",
      "0.0033012\n",
      "0.0033784\n",
      "0.0033025\n",
      "0.0033571\n",
      "0.0034205\n",
      "0.0033802\n",
      "0.0034497\n",
      "0.0033826\n",
      "0.0033388\n",
      "0.0033557\n",
      "0.0033761\n",
      "0.0034014\n",
      "0.0033595\n",
      "0.0033705\n",
      "0.0033588\n",
      "0.0033427\n",
      "0.0033573\n",
      "0.0034389\n",
      "0.0033593\n",
      "0.0034198\n",
      "0.0032967\n",
      "0.0033735\n",
      "0.0033414\n",
      "0.0033924\n",
      "0.0034190\n",
      "0.0033476\n",
      "0.0033886\n",
      "0.0034408\n",
      "0.0033804\n",
      "0.0033356\n",
      "0.0033709\n",
      "0.0033495\n",
      "0.0034705\n",
      "0.0034565\n",
      "0.0033770\n",
      "0.0034308\n",
      "0.0033981\n",
      "0.0033691\n",
      "0.0034026\n",
      "0.0034365\n",
      "0.0033463\n",
      "0.0034294\n",
      "0.0033897\n",
      "0.0033470\n",
      "0.0034409\n",
      "0.0033577\n",
      "0.0033621\n",
      "0.0032536\n",
      "0.0033941\n",
      "0.0035104\n",
      "0.0033858\n",
      "0.0033372\n",
      "0.0034185\n",
      "0.0033283\n",
      "0.0033970\n",
      "0.0033605\n",
      "0.0034555\n",
      "0.0034710\n",
      "0.0033091\n",
      "0.0033207\n",
      "0.0034208\n",
      "0.0033302\n",
      "0.0033129\n",
      "0.0033817\n",
      "0.0033145\n",
      "0.0033616\n",
      "0.0032608\n",
      "0.0033605\n",
      "0.0033925\n",
      "0.0033303\n",
      "0.0032977\n",
      "0.0033642\n",
      "0.0033428\n",
      "0.0033359\n",
      "0.0033626\n",
      "0.0034378\n",
      "0.0034087\n",
      "0.0034590\n",
      "0.0033741\n",
      "0.0033989\n",
      "0.0033275\n",
      "0.0033763\n",
      "0.0034328\n",
      "0.0033386\n",
      "0.0033964\n",
      "0.0032997\n",
      "0.0033337\n",
      "0.0033813\n",
      "0.0033911\n",
      "0.0033912\n",
      "0.0034004\n",
      "0.0034403\n",
      "0.0034060\n",
      "0.0033330\n",
      "0.0033970\n",
      "0.0034119\n",
      "0.0033570\n",
      "0.0034332\n",
      "0.0034070\n",
      "0.0033824\n",
      "0.0033352\n",
      "0.0033437\n",
      "0.0033932\n",
      "0.0034469\n",
      "0.0034097\n",
      "0.0033408\n",
      "0.0033822\n",
      "0.0033384\n",
      "0.0033124\n",
      "0.0033017\n",
      "0.0034363\n",
      "0.0033614\n",
      "0.0033914\n",
      "0.0033448\n",
      "0.0033558\n",
      "0.0033793\n",
      "0.0033528\n",
      "0.0034247\n",
      "0.0034003\n",
      "0.0033859\n",
      "0.0034155\n",
      "0.0033849\n",
      "0.0033983\n",
      "0.0033475\n",
      "0.0034157\n",
      "0.0034163\n",
      "0.0032835\n",
      "0.0033604\n",
      "0.0033196\n",
      "0.0033448\n",
      "0.0033663\n",
      "0.0033930\n",
      "0.0033823\n",
      "0.0033631\n",
      "0.0033744\n",
      "0.0034129\n",
      "0.0033652\n",
      "0.0033531\n",
      "0.0034503\n",
      "0.0034931\n",
      "0.0033746\n",
      "0.0034616\n",
      "0.0033390\n",
      "0.0033820\n",
      "0.0034318\n",
      "0.0033880\n",
      "0.0033962\n",
      "0.0033582\n",
      "0.0034034\n",
      "0.0034187\n",
      "0.0034269\n",
      "0.0033217\n",
      "0.0033629\n",
      "0.0033376\n",
      "0.0033528\n",
      "0.0033551\n",
      "0.0034155\n",
      "0.0033650\n",
      "0.0034212\n",
      "0.0033542\n",
      "0.0033806\n",
      "0.0033604\n",
      "0.0033253\n",
      "0.0033682\n",
      "0.0033745\n",
      "0.0034526\n",
      "0.0033989\n",
      "0.0033589\n",
      "0.0034095\n",
      "0.0033703\n",
      "0.0033850\n",
      "0.0033887\n",
      "0.0033983\n",
      "0.0032515\n",
      "0.0033165\n",
      "0.0033361\n",
      "0.0034859\n",
      "0.0033656\n",
      "0.0033583\n",
      "0.0033926\n",
      "0.0033270\n",
      "0.0033493\n",
      "0.0033542\n",
      "0.0033388\n",
      "0.0033896\n",
      "0.0034788\n",
      "0.0033592\n",
      "0.0033236\n",
      "0.0033244\n",
      "0.0033640\n",
      "0.0033666\n",
      "0.0033776\n",
      "0.0034530\n",
      "0.0034145\n",
      "0.0034070\n",
      "0.0034050\n",
      "0.0034130\n",
      "0.0034192\n",
      "0.0033843\n",
      "0.0033831\n",
      "0.0034260\n",
      "0.0033325\n",
      "0.0034319\n",
      "0.0034481\n",
      "0.0033168\n",
      "0.0033270\n",
      "0.0033985\n",
      "0.0034349\n",
      "0.0033409\n",
      "0.0034595\n",
      "0.0033410\n",
      "0.0034241\n",
      "0.0033763\n",
      "0.0033155\n",
      "0.0033217\n",
      "0.0034261\n",
      "0.0033604\n",
      "0.0034553\n",
      "0.0033484\n",
      "0.0033325\n",
      "0.0033522\n",
      "0.0033840\n",
      "0.0033093\n",
      "0.0033162\n",
      "0.0033372\n",
      "0.0033676\n",
      "0.0034137\n",
      "0.0033921\n",
      "0.0033050\n",
      "0.0032625\n",
      "0.0034612\n",
      "0.0033229\n",
      "0.0033019\n",
      "0.0033025\n",
      "0.0033242\n",
      "0.0034674\n",
      "0.0033322\n",
      "0.0034114\n",
      "0.0033442\n",
      "0.0033469\n",
      "0.0033694\n",
      "0.0034068\n",
      "0.0033968\n",
      "0.0033766\n",
      "0.0033734\n",
      "0.0033807\n",
      "0.0033950\n",
      "0.0034167\n",
      "0.0033649\n",
      "0.0034115\n",
      "0.0034320\n",
      "0.0034156\n",
      "0.0033469\n",
      "0.0033553\n",
      "0.0033748\n",
      "0.0033104\n",
      "0.0034172\n",
      "0.0033190\n",
      "0.0033191\n",
      "0.0034093\n",
      "0.0033492\n",
      "0.0034092\n",
      "0.0033631\n",
      "0.0033588\n",
      "0.0033933\n",
      "0.0033401\n",
      "0.0033483\n",
      "0.0032900\n",
      "0.0034116\n",
      "0.0033800\n",
      "0.0033472\n",
      "0.0033563\n",
      "0.0033875\n",
      "0.0033360\n",
      "0.0033814\n",
      "0.0033783\n",
      "0.0034202\n",
      "0.0033692\n",
      "0.0034040\n",
      "0.0033994\n",
      "0.0033665\n",
      "0.0033796\n",
      "0.0033718\n",
      "0.0033994\n",
      "0.0033381\n",
      "0.0034117\n",
      "0.0034043\n",
      "0.0034286\n",
      "0.0033575\n",
      "0.0034464\n",
      "0.0032801\n",
      "0.0033139\n",
      "0.0033218\n",
      "0.0032976\n",
      "0.0034169\n",
      "0.0034565\n",
      "0.0034408\n",
      "0.0033953\n",
      "0.0034485\n",
      "0.0033247\n",
      "0.0033909\n",
      "0.0034178\n",
      "0.0033719\n",
      "0.0034472\n",
      "0.0034259\n",
      "0.0033457\n",
      "0.0033986\n",
      "0.0033576\n",
      "0.0033073\n",
      "0.0034317\n",
      "0.0034793\n",
      "0.0033516\n",
      "0.0034014\n",
      "0.0033918\n",
      "0.0034037\n",
      "0.0034114\n",
      "0.0034169\n",
      "0.0033925\n",
      "0.0032782\n",
      "0.0033566\n",
      "0.0033720\n",
      "0.0033892\n",
      "0.0033367\n",
      "0.0034028\n",
      "0.0034048\n",
      "0.0033704\n",
      "0.0034355\n",
      "0.0033823\n",
      "0.0034243\n",
      "0.0034546\n",
      "0.0033554\n",
      "0.0033572\n",
      "0.0034390\n",
      "0.0033838\n",
      "0.0033433\n",
      "0.0034188\n",
      "0.0034431\n",
      "0.0035308\n",
      "0.0033121\n",
      "0.0033191\n",
      "0.0034658\n",
      "0.0034055\n",
      "0.0034771\n",
      "0.0033353\n",
      "0.0033557\n",
      "0.0033667\n",
      "0.0034119\n",
      "0.0033509\n",
      "0.0033574\n",
      "0.0034847\n",
      "0.0034734\n",
      "0.0034207\n",
      "0.0033496\n",
      "0.0034424\n",
      "0.0033103\n",
      "0.0032842\n",
      "0.0034615\n",
      "0.0033351\n",
      "0.0033625\n",
      "0.0033286\n",
      "0.0033848\n",
      "0.0033868\n",
      "0.0032853\n",
      "0.0033896\n",
      "0.0033629\n",
      "0.0034003\n",
      "0.0033855\n",
      "0.0033738\n",
      "0.0034243\n",
      "0.0033607\n",
      "0.0033123\n",
      "0.0034174\n",
      "0.0033543\n",
      "0.0032200\n",
      "0.0033759\n",
      "0.0034000\n",
      "0.0034033\n",
      "0.0033006\n",
      "0.0034093\n",
      "0.0033814\n",
      "0.0033637\n",
      "0.0033684\n",
      "0.0034268\n",
      "0.0033858\n",
      "0.0033553\n",
      "0.0034097\n",
      "0.0033143\n",
      "0.0032904\n",
      "0.0034241\n",
      "0.0034836\n",
      "0.0033916\n",
      "0.0034375\n",
      "0.0033521\n",
      "0.0033580\n",
      "0.0033791\n",
      "0.0034640\n",
      "0.0034737\n",
      "0.0032864\n",
      "0.0033852\n",
      "0.0034102\n",
      "0.0033310\n",
      "0.0033801\n",
      "0.0033575\n",
      "0.0033372\n",
      "0.0034715\n",
      "0.0033582\n",
      "0.0033302\n",
      "0.0032968\n",
      "0.0034074\n",
      "0.0033937\n",
      "0.0033925\n",
      "0.0033236\n",
      "0.0032748\n",
      "0.0033760\n",
      "0.0033667\n",
      "0.0034170\n",
      "0.0034495\n",
      "0.0033530\n",
      "0.0033730\n",
      "0.0034255\n",
      "0.0033937\n",
      "0.0033147\n",
      "0.0033684\n",
      "0.0033711\n",
      "0.0032703\n",
      "0.0033694\n",
      "0.0033772\n",
      "0.0033464\n",
      "0.0034174\n",
      "0.0033745\n",
      "0.0033728\n",
      "0.0034373\n",
      "0.0033567\n",
      "0.0033852\n",
      "0.0034357\n",
      "0.0033153\n",
      "0.0033923\n",
      "0.0034442\n",
      "0.0033868\n",
      "0.0034305\n",
      "0.0032823\n",
      "0.0034344\n",
      "0.0034132\n",
      "0.0033976\n",
      "0.0033676\n",
      "0.0032986\n",
      "0.0034304\n",
      "0.0033377\n",
      "0.0033774\n",
      "0.0033680\n",
      "0.0034647\n",
      "0.0033965\n",
      "0.0033961\n",
      "0.0033583\n",
      "0.0033890\n",
      "0.0033407\n",
      "0.0032996\n",
      "0.0033688\n",
      "0.0034054\n",
      "0.0034152\n",
      "0.0033957\n",
      "0.0033840\n",
      "0.0033946\n",
      "0.0033911\n",
      "0.0032927\n",
      "0.0032354\n",
      "0.0033713\n",
      "0.0034044\n",
      "0.0033673\n",
      "0.0033537\n",
      "0.0033782\n",
      "0.0033632\n",
      "0.0034180\n",
      "0.0033483\n",
      "0.0033809\n",
      "0.0034331\n",
      "0.0034289\n",
      "0.0033239\n",
      "0.0034208\n",
      "0.0035350\n",
      "0.0033258\n",
      "0.0034332\n",
      "0.0033245\n",
      "0.0034452\n",
      "0.0033473\n",
      "0.0033891\n",
      "0.0033673\n",
      "0.0033914\n",
      "0.0033642\n",
      "0.0033711\n",
      "0.0033920\n",
      "0.0033311\n",
      "0.0033758\n",
      "0.0034222\n",
      "0.0033873\n",
      "0.0034244\n",
      "0.0034058\n",
      "0.0033874\n",
      "0.0034556\n",
      "0.0032950\n",
      "0.0033580\n",
      "0.0033787\n",
      "0.0033526\n",
      "0.0034633\n",
      "0.0033055\n",
      "0.0033699\n",
      "0.0033565\n",
      "0.0034361\n",
      "0.0034799\n",
      "0.0033772\n",
      "0.0034013\n",
      "0.0034192\n",
      "0.0033999\n",
      "0.0034013\n",
      "0.0034449\n",
      "0.0031643\n",
      "0.0033348\n",
      "0.0032272\n",
      "0.0034014\n",
      "0.0034154\n",
      "0.0034713\n",
      "0.0034286\n",
      "0.0033502\n",
      "0.0033323\n",
      "0.0033723\n",
      "0.0034014\n",
      "0.0034126\n",
      "0.0033807\n",
      "0.0034395\n",
      "0.0033963\n",
      "0.0033441\n",
      "0.0033969\n",
      "0.0033316\n",
      "0.0032979\n",
      "0.0033388\n",
      "0.0033573\n",
      "0.0033440\n",
      "0.0034383\n",
      "0.0033730\n",
      "0.0034288\n",
      "0.0033640\n",
      "0.0034028\n",
      "0.0033779\n",
      "0.0033957\n",
      "0.0033399\n",
      "0.0033713\n",
      "0.0033695\n",
      "0.0033513\n",
      "0.0033217\n",
      "0.0033499\n",
      "0.0033390\n",
      "0.0033796\n",
      "0.0034255\n",
      "0.0034383\n",
      "0.0035655\n",
      "0.0033782\n",
      "0.0033202\n",
      "0.0033730\n",
      "0.0033742\n",
      "0.0034220\n",
      "0.0033696\n",
      "0.0034671\n",
      "0.0034281\n",
      "0.0033597\n",
      "0.0033714\n",
      "0.0033692\n",
      "0.0033473\n",
      "0.0034347\n",
      "0.0033693\n",
      "0.0034090\n",
      "0.0033767\n",
      "0.0034053\n",
      "0.0034642\n",
      "0.0034274\n",
      "0.0034153\n",
      "0.0033523\n",
      "0.0033787\n",
      "0.0033564\n",
      "0.0034127\n",
      "0.0034290\n",
      "0.0034251\n",
      "0.0033932\n",
      "0.0033443\n",
      "0.0033374\n",
      "0.0033958\n",
      "0.0033340\n",
      "0.0033874\n",
      "0.0033305\n",
      "0.0033956\n",
      "0.0033705\n",
      "0.0033883\n",
      "0.0033906\n",
      "0.0034310\n",
      "0.0034044\n",
      "0.0034688\n",
      "0.0033855\n",
      "0.0034056\n",
      "0.0032748\n",
      "0.0034031\n",
      "0.0034616\n",
      "0.0034125\n",
      "0.0033746\n",
      "0.0033295\n",
      "0.0033510\n",
      "0.0032785\n",
      "0.0034253\n",
      "0.0034094\n",
      "0.0034253\n",
      "0.0033537\n",
      "0.0033811\n",
      "0.0033549\n",
      "0.0034317\n",
      "0.0034019\n",
      "0.0033503\n",
      "0.0033361\n",
      "0.0034306\n",
      "0.0033507\n",
      "0.0033304\n",
      "0.0033428\n",
      "0.0033831\n",
      "0.0034786\n",
      "0.0033133\n",
      "0.0034756\n",
      "0.0033678\n",
      "0.0033117\n",
      "0.0033507\n",
      "0.0034305\n",
      "0.0033454\n",
      "0.0034298\n",
      "0.0034340\n",
      "0.0034340\n",
      "0.0034100\n",
      "0.0033809\n",
      "0.0034285\n",
      "0.0034483\n",
      "0.0034256\n",
      "0.0034037\n",
      "0.0033446\n",
      "0.0034263\n",
      "0.0033092\n",
      "0.0034020\n",
      "0.0032589\n",
      "0.0033515\n",
      "0.0033547\n",
      "0.0032890\n",
      "0.0034287\n",
      "0.0033446\n",
      "0.0034164\n",
      "0.0033870\n",
      "0.0033706\n",
      "0.0033853\n",
      "0.0034015\n",
      "0.0033033\n",
      "0.0033243\n",
      "0.0034144\n",
      "0.0033930\n",
      "0.0033891\n",
      "0.0033259\n",
      "0.0033620\n",
      "0.0033491\n",
      "0.0034245\n",
      "0.0034130\n",
      "0.0033398\n",
      "0.0033897\n",
      "0.0033942\n",
      "0.0033431\n",
      "0.0033369\n",
      "0.0034275\n",
      "0.0033563\n",
      "0.0033422\n",
      "0.0033728\n",
      "0.0034144\n",
      "0.0034272\n",
      "0.0033606\n",
      "0.0033822\n",
      "0.0033709\n",
      "0.0033856\n",
      "0.0034093\n",
      "0.0034128\n",
      "0.0033536\n",
      "0.0032951\n",
      "0.0034183\n",
      "0.0033671\n",
      "0.0033690\n",
      "0.0033726\n",
      "0.0034118\n",
      "0.0033848\n",
      "0.0033804\n",
      "0.0033324\n",
      "0.0034098\n",
      "0.0033879\n",
      "0.0033734\n",
      "0.0034245\n",
      "0.0033991\n",
      "0.0034220\n",
      "0.0033523\n",
      "0.0033754\n",
      "0.0033311\n",
      "0.0033216\n",
      "0.0034120\n",
      "0.0034106\n",
      "0.0033535\n",
      "0.0033073\n",
      "0.0034040\n",
      "0.0033269\n",
      "0.0033134\n",
      "0.0033467\n",
      "0.0033702\n",
      "0.0033209\n",
      "0.0034333\n",
      "0.0033678\n",
      "0.0033154\n",
      "0.0033338\n",
      "0.0034228\n",
      "0.0034209\n",
      "0.0033972\n",
      "0.0033349\n",
      "0.0034407\n",
      "0.0033186\n",
      "0.0033522\n",
      "0.0034232\n",
      "0.0033486\n",
      "0.0034713\n",
      "0.0033536\n",
      "0.0034459\n",
      "0.0033546\n",
      "0.0033583\n",
      "0.0032702\n",
      "0.0033585\n",
      "0.0033649\n",
      "0.0034580\n",
      "0.0032675\n",
      "0.0033605\n",
      "0.0033320\n",
      "0.0033882\n",
      "0.0034048\n",
      "0.0033042\n",
      "0.0034541\n",
      "0.0033688\n",
      "0.0033753\n",
      "0.0033461\n",
      "0.0034253\n",
      "0.0034454\n",
      "0.0034405\n",
      "0.0033685\n",
      "0.0033717\n",
      "0.0033152\n",
      "0.0033328\n",
      "0.0033149\n",
      "0.0033937\n",
      "0.0033576\n",
      "0.0034004\n",
      "0.0033709\n",
      "0.0033512\n",
      "0.0034033\n",
      "0.0034106\n",
      "0.0033800\n",
      "0.0033808\n",
      "0.0034163\n",
      "0.0032917\n",
      "0.0034232\n",
      "0.0033077\n",
      "0.0033698\n",
      "0.0033611\n",
      "0.0033703\n",
      "0.0034499\n",
      "0.0033654\n",
      "0.0033034\n",
      "0.0033747\n",
      "0.0033610\n",
      "0.0032908\n",
      "0.0033692\n",
      "0.0032836\n",
      "0.0033627\n",
      "0.0033324\n",
      "0.0034546\n",
      "0.0033774\n",
      "0.0033464\n",
      "0.0034255\n",
      "0.0033898\n",
      "0.0034092\n",
      "0.0034244\n",
      "0.0033902\n",
      "0.0033497\n",
      "0.0033555\n",
      "0.0033597\n",
      "0.0034073\n",
      "0.0034289\n",
      "0.0032925\n",
      "0.0032835\n",
      "0.0034661\n",
      "0.0033879\n",
      "0.0034190\n",
      "0.0034029\n",
      "0.0033757\n",
      "0.0034093\n",
      "0.0033896\n",
      "0.0033899\n",
      "0.0033831\n",
      "0.0033889\n",
      "0.0033917\n",
      "0.0032568\n",
      "0.0033276\n",
      "0.0033408\n",
      "0.0032774\n",
      "0.0034213\n",
      "0.0033141\n",
      "0.0034298\n",
      "0.0034649\n",
      "0.0033660\n",
      "0.0033641\n",
      "0.0033164\n",
      "0.0033280\n",
      "0.0033891\n",
      "0.0033992\n",
      "0.0034288\n",
      "0.0034022\n",
      "0.0033160\n",
      "0.0034873\n",
      "0.0034774\n",
      "0.0033836\n",
      "0.0033008\n",
      "0.0034509\n",
      "0.0033859\n",
      "0.0033679\n",
      "0.0033912\n",
      "0.0033852\n",
      "0.0034431\n",
      "0.0034294\n",
      "0.0033749\n",
      "0.0034044\n",
      "0.0033470\n",
      "0.0033957\n",
      "0.0032840\n",
      "0.0034015\n",
      "0.0034355\n",
      "0.0033068\n",
      "0.0034355\n",
      "0.0034496\n",
      "0.0034154\n",
      "0.0033595\n",
      "0.0032967\n",
      "0.0033460\n",
      "0.0033401\n",
      "0.0033636\n",
      "0.0033984\n",
      "0.0033762\n",
      "0.0034086\n",
      "0.0033186\n",
      "0.0033606\n",
      "0.0034509\n",
      "0.0034045\n",
      "0.0032464\n",
      "0.0033208\n",
      "0.0033233\n",
      "0.0035095\n",
      "0.0034199\n",
      "0.0033178\n",
      "0.0033876\n",
      "0.0033461\n",
      "0.0034588\n",
      "0.0033444\n",
      "0.0033409\n",
      "0.0033661\n",
      "0.0033635\n",
      "0.0034275\n",
      "0.0033421\n",
      "0.0033047\n",
      "0.0033547\n",
      "0.0033718\n",
      "0.0033364\n",
      "0.0034107\n",
      "0.0033003\n",
      "0.0034065\n",
      "0.0034287\n",
      "0.0034442\n",
      "0.0033765\n",
      "0.0033915\n",
      "0.0033248\n",
      "0.0035086\n",
      "0.0033179\n",
      "0.0032974\n",
      "0.0034327\n",
      "0.0033409\n",
      "0.0033808\n",
      "0.0033225\n",
      "0.0033762\n",
      "0.0033151\n",
      "0.0033682\n",
      "0.0033885\n",
      "0.0033569\n",
      "0.0033724\n",
      "0.0033526\n",
      "0.0033574\n",
      "0.0033194\n"
     ]
    }
   ],
   "source": [
    "SNR_min_dB  = 0\n",
    "SNR_max_dB  = 20\n",
    "step_dB     = 2\n",
    "num_dB      = int((SNR_max_dB - SNR_min_dB) / step_dB) + 1\n",
    "\n",
    "SNR         = np.linspace(SNR_min_dB, SNR_max_dB, num=num_dB)\n",
    "log         = r'C:\\Users\\SON\\Desktop\\ISDNN\\Python\\Unstructured\\model_detnet_10k_4l_1\\log_test.txt'\n",
    "\n",
    "N_test = int(100)\n",
    "\n",
    "for i in range (100):\n",
    "    for snr in SNR:\n",
    "        # with Record(log):\n",
    "        #     print(snr)\n",
    "        DataSet_x, DataSet_y, DataSet_H, DataSet_HH = Gen_dataset('test', snr, 0, N_test)\n",
    "        H_true, H_raw, H_in, v, xTx, xTy = Input_ISDNN('test', DataSet_x, DataSet_y, DataSet_H, DataSet_HH, N_test)\n",
    "        \n",
    "        # LS(DataSet_x, DataSet_y)\n",
    "        # MMSE(DataSet_x, DataSet_y, snr, H_raw)\n",
    "        test(H_raw, H_in, v, xTx, xTy, N_test, log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
