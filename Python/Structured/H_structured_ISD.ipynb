{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSmcwLZuptDU"
   },
   "source": [
    "# Proposed Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG5fZn_ypunP"
   },
   "source": [
    "# **Biblioheque**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "--SvzvIspu-U"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from scipy.stats import rice\n",
    "# import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import sys\n",
    "import timeit\n",
    "import os\n",
    "\n",
    "# torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBk01Vu0pvMm"
   },
   "source": [
    "# class to save results in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "06IymVCkpvYS"
   },
   "outputs": [],
   "source": [
    "class Record:\n",
    "    def __init__(self, TextName):\n",
    "        self.out_file = open(TextName, 'a')\n",
    "        self.old_stdout = sys.stdout\n",
    "        sys.stdout = self\n",
    "\n",
    "    def write(self, text):\n",
    "        self.old_stdout.write(text)\n",
    "        self.out_file.write(text)\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout = self.old_stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xPamGjNpv8E"
   },
   "source": [
    "# **slicer the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "dlQdA0xHpwF7"
   },
   "outputs": [],
   "source": [
    "def slicer(data):\n",
    "    dataI = data[slice(0, len(data), 2)]\n",
    "    dataQ = data[slice(1, len(data), 2)]\n",
    "    return(dataI, dataQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOoiIo_LpwQl"
   },
   "source": [
    "# **Modulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "xIftC8fvpwir"
   },
   "outputs": [],
   "source": [
    "def mapper_16QAM(QAM16, data):\n",
    "    map0 = 2*data[slice(0, len(data), 2)] + data[slice(1, len(data), 2)]\n",
    "    map0 = list(map(int, map0))\n",
    "    dataMapped = []\n",
    "    for i in range(len(map0)):\n",
    "        dataMapped.append(QAM16[map0[i]])\n",
    "    return(dataMapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "AYhPgAIcqtGT"
   },
   "outputs": [],
   "source": [
    "def calculate_bits(Modulation,NumSubcarriers,NumDataSymb):\n",
    "    if Modulation=='QPSK':\n",
    "        Nbpscs=2\n",
    "    elif Modulation=='16QAM':\n",
    "        Nbpscs=4\n",
    "    return NumDataSymb*NumSubcarriers*Nbpscs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgX6tiqFpwvb"
   },
   "source": [
    "# **generate noise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "2LQwZvGaprKZ"
   },
   "outputs": [],
   "source": [
    "def AWGN(IFsig, SNR):\n",
    "    dP = np.zeros(len(IFsig))\n",
    "    P = 0\n",
    "\n",
    "    for i in range(len(IFsig)):\n",
    "        dP[i] = abs(IFsig[i])**2\n",
    "        P = P + dP[i]\n",
    "\n",
    "    P = P/len(IFsig)\n",
    "    gamma = 10**(SNR/10)\n",
    "    N0 = P/gamma\n",
    "    n = ((N0/2)**(0.5))*np.random.standard_normal(len(IFsig))\n",
    "    IF_n = np.zeros((len(IFsig),1))\n",
    "\n",
    "    for i in range(len(IFsig)):\n",
    "        IF_n[i,:] = IFsig[i] + n[i]\n",
    "\n",
    "    return(IF_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate channel model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unstructured_channel(Nt, Nr, type):\n",
    "    if (type == 'gauss'):\n",
    "        return (np.random.normal(size=(Nt, Nr))+1j*np.random.normal(size=(Nt, Nr)))/np.sqrt(2)\n",
    "    \n",
    "def Generate_channel(Nt, Nr, d_H, array_type, elements_nor, type, Nr_ULA, Nr_UCA):\n",
    "    if (type == 'gauss'):\n",
    "\n",
    "        H = (np.random.normal(size=(Nt, Nr))+1j*np.random.normal(size=(Nt, Nr)))/np.sqrt(2)\n",
    "        H_oG = np.zeros((Nt, Nr), dtype=complex)\n",
    "        Gamma = []\n",
    "        Zoa = []\n",
    "        Aoa = []\n",
    "\n",
    "        Steering = np.zeros((Nt, Nr, d_H), dtype=complex)\n",
    "\n",
    "        for nt in range (Nt):\n",
    "            # gamma = (np.random.normal(size=d_H)+1j*np.random.normal(size=d_H))/np.sqrt(2)\n",
    "            zoa = np.random.uniform(-np.pi / 2, np.pi / 2, d_H)\n",
    "            aoa = np.random.uniform(-np.pi / 2, np.pi / 2, d_H)\n",
    "            \n",
    "            # Gamma.append(gamma)\n",
    "            Zoa.append(zoa)\n",
    "            Aoa.append(aoa)\n",
    "\n",
    "        # Gamma = np.array(Gamma)\n",
    "        Zoa = np.array(Zoa)\n",
    "        Aoa = np.array(Aoa)\n",
    "\n",
    "        if array_type == 'ULA':\n",
    "            for nt in range (Nt):\n",
    "                for nr in range (Nr):       \n",
    "                    h = 0\n",
    "                    for dh in range(d_H): \n",
    "                        r_x = np.sin(Zoa[nt,dh]) * np.cos(Aoa[nt,dh])\n",
    "                        r_y = np.sin(Zoa[nt,dh]) * np.sin(Aoa[nt,dh])\n",
    "                        r_z = np.cos(Zoa[nt,dh])\n",
    "\n",
    "                        # h = h + Gamma[nn,dh] * np.exp(-1j*2*np.pi * (elements_nor[0, 0, nr]*r_x \n",
    "                        #                                             + elements_nor[1, 0, nr]*r_y \n",
    "                        #                                             + elements_nor[2, 0, nr]*r_z))\n",
    "                        \n",
    "                        Steering[nt, nr, dh] = np.exp(-1j*2*np.pi * (elements_nor[0, 0, nr]*r_x \n",
    "                                                                    + elements_nor[1, 0, nr]*r_y \n",
    "                                                                    + elements_nor[2, 0, nr]*r_z))\n",
    "                    \n",
    "                    H_oG[nt, nr] = np.divide(H[nt, nr],  Steering[nt, nr, 0]) \n",
    "\n",
    "                    # H[nr, nn] = h\n",
    "        else:\n",
    "            for nt in range (Nt):\n",
    "                r = -1\n",
    "                for Nr_ULA_index in range (Nr_ULA):\n",
    "                    for Nr_UCA_index in range (Nr_UCA):\n",
    "                        r=r+1\n",
    "                        h = 0\n",
    "                        for dh in range(d_H): \n",
    "                            r_x = np.sin(Zoa[nt, dh]) * np.cos(Aoa[nt, dh])\n",
    "                            r_y = np.sin(Zoa[nt, dh]) * np.sin(Aoa[nt, dh])\n",
    "                            r_z = np.cos(Zoa[nt, dh])\n",
    "\n",
    "                            # h = h + Gamma[nt, dh] * np.exp(-1j*2*np.pi *  (elements_nor[0, Nr_ULA_index, Nr_UCA_index]*r_x \n",
    "                            #                                             + elements_nor[1, Nr_ULA_index, Nr_UCA_index]*r_y \n",
    "                            #                                             + elements_nor[2, Nr_ULA_index, Nr_UCA_index]*r_z))\n",
    "                                                                        \n",
    "                            Steering[nt, r, dh] = np.exp(-1j*2*np.pi *(elements_nor[0, Nr_ULA_index, Nr_UCA_index]*r_x \n",
    "                                                                        + elements_nor[1, Nr_ULA_index, Nr_UCA_index]*r_y \n",
    "                                                                        + elements_nor[2, Nr_ULA_index, Nr_UCA_index]*r_z))\n",
    "\n",
    "                        # tmp_1 = 0\n",
    "                        # gamma = 0\n",
    "                        # while(tmp_1 < 0.6):\n",
    "                        #     gamma = (np.random.normal(size=d_H - 1)+1j*np.random.normal(size=d_H - 1))/np.sqrt(2)  \n",
    "                        #     tmp = 0\n",
    "                        #     for jj in range (d_H - 1):\n",
    "                        #         tmp += gamma[jj] * Steering[r, nt, jj]\n",
    "                        #     last_gamma = np.divide(H[r, nt] - tmp, Steering[r, nt, d_H - 1])\n",
    "                        #     gamma = np.append(gamma, last_gamma)\n",
    "\n",
    "                        #     tmp_1 = min(abs(gamma))\n",
    "                        \n",
    "                        H_oG[nt, r] = np.divide(H[nt, r],  Steering[nt, r, 0]) \n",
    "                        # H[r, nt] = h\n",
    "\n",
    "        return H, H_oG, Gamma, Steering\n",
    "    \n",
    "    if (type == 'rayleigh'):\n",
    "        return (np.random.rayleigh(scale=(1/np.sqrt(2)), size=(Nt, Nr)) + 1j*np.random.rayleigh(scale=(1/np.sqrt(2)), size=(Nt, Nr)))/np.sqrt(2)\n",
    "    if (type == 'rician'):\n",
    "        b = 1/np.sqrt(2)\n",
    "        return (rice.rvs(b, size=(Nt, Nr)) + 1j*rice.rvs(b, size=(Nt, Nr)))/np.sqrt(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eciNtnFjq2yd"
   },
   "source": [
    "# **Generate Dataset**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "AE4Q6jZCq3CY"
   },
   "outputs": [],
   "source": [
    "DataSet_x   = []  # x dataset after modulation\n",
    "DataSet_y   = []  # y dataset\n",
    "DataSet_HH  = []  # H dataset\n",
    "DataSet_b   = []  # binary dataset\n",
    "SNR_min_dB  = 0\n",
    "SNR_max_dB  = 20\n",
    "step_dB     = 5\n",
    "num_dB      = int((SNR_max_dB - SNR_min_dB) / step_dB) + 1\n",
    "\n",
    "SNR         = np.linspace(SNR_min_dB, SNR_max_dB, num=num_dB)\n",
    "\n",
    "\n",
    "Nt = 8             # Tx: 8\n",
    "Nr = 64            # Rx: 128\n",
    "N_samp = 100\n",
    "\n",
    "\n",
    "def Gen_dataset(mode, array_type, snr, imperfect, N_samp):    \n",
    "    DataSet_x   = []  # x dataset after modulation\n",
    "    DataSet_y   = []  # y dataset\n",
    "    DataSet_H   = []  \n",
    "    DataSet_HH  = []\n",
    "    DataSet_Steering = []\n",
    "\n",
    "    NumSubcarriers = 1\n",
    "    Modulation = '16QAM'\n",
    "    QAM16 = [-1, -0.333, 0.333, 1]\n",
    "    NumDataSymb = 1\n",
    "    N_type = 'gauss'\n",
    "\n",
    "    d_H = 1\n",
    "    d_ULA_nor  = 0.5\n",
    "    d_UCA_nor  = 0.5\n",
    "    Nr_UCA = 16\n",
    "    Nr_ULA = 4\n",
    "\n",
    "    if array_type == 'ULA':\n",
    "        elements_nor = np.zeros((3, 1, Nr), dtype=float)\n",
    "        for Nr_index in range (Nr):\n",
    "            elements_nor[0, 0, Nr_index] = (Nr_index-1) * d_UCA_nor\n",
    "            elements_nor[1, 0, Nr_index] = 0\n",
    "            elements_nor[2, 0, Nr_index] = 0\n",
    "    else:\n",
    "        elements_nor = np.zeros((3, Nr_ULA, Nr_UCA), dtype=float)\n",
    "\n",
    "        R_nor = 0.5 * d_UCA_nor / np.sin(np.pi/Nr_UCA)\n",
    "\n",
    "        for Nr_ULA_index in range (Nr_ULA):\n",
    "            for Nr_UCA_index in range (Nr_UCA):\n",
    "                elements_nor[0, Nr_ULA_index, Nr_UCA_index] = R_nor * np.sin((Nr_UCA_index-1)*(2*np.pi/Nr_UCA)) ;         \n",
    "                elements_nor[1, Nr_ULA_index, Nr_UCA_index] = R_nor * np.cos((Nr_UCA_index-1)*(2*np.pi/Nr_UCA)) ;         \n",
    "                elements_nor[2, Nr_ULA_index, Nr_UCA_index] = (Nr_ULA_index-1) * d_ULA_nor;    \n",
    "\n",
    "    if mode == 'train':\n",
    "        for snr in SNR:\n",
    "            for runIdx in range(0, N_samp):      # ! 20000 x Nt: samples\n",
    "                H, H_oG, Gamma, Steering = Generate_channel(Nt, Nr, d_H, array_type, elements_nor, N_type, Nr_ULA, Nr_UCA)\n",
    "                \n",
    "                HH = np.concatenate((np.concatenate((H_oG.real, H_oG.imag), axis=1),\n",
    "                                    np.concatenate((-H_oG.imag, H_oG.real), axis=1)), axis=0)\n",
    "                \n",
    "                x = np.zeros((2*Nt, NumSubcarriers))\n",
    "                a = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
    "                DataRaw = np.zeros((Nt, a))\n",
    "                for t in range(Nt):\n",
    "                    #\"data symbol generate\"\n",
    "                    NumBits = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
    "                    bit = np.random.randint(1, 3, NumBits)-1\n",
    "                    DataRaw[t, :] = bit\n",
    "                    for j in range(4):\n",
    "                        DataSet_b.append(bit[j])\n",
    "                    I = np.zeros((1, a))\n",
    "                    I[0, :] = DataRaw[t, :]\n",
    "                    (dataI, dataQ) = slicer(I[0])\n",
    "\n",
    "                    # Mapper\n",
    "                    mapI = mapper_16QAM(QAM16, dataI)\n",
    "                    mapQ = mapper_16QAM(QAM16, dataQ)\n",
    "                    x[t] = mapI[0]\n",
    "                    x[t+Nt] = mapQ[0]\n",
    "\n",
    "                # transpose\n",
    "                x = x.transpose()\n",
    "\n",
    "                y_wo_noise = np.matmul(x, HH)\n",
    "\n",
    "                # noise\n",
    "                noise = AWGN(y_wo_noise.transpose(), snr)\n",
    "\n",
    "                y = y_wo_noise + noise.transpose()\n",
    "\n",
    "                DataSet_x.append(x)    # ! I, Q sample distance by Nt.\n",
    "                DataSet_y.append(y)                 # ! output sample\n",
    "                \n",
    "                # Imperfect channel: 5%\n",
    "                # coef = (2*np.random.randint(0,2,size=HH.shape) - 1)\n",
    "                # HH = HH + coef * HH * 0.05\n",
    "                DataSet_Steering.append(np.squeeze(Steering))\n",
    "                DataSet_HH.append(HH)\n",
    "                DataSet_H.append(H)               # ! Generated channel\n",
    "                \n",
    "    else:\n",
    "        for runIdx in range(0, N_samp):      # ! 20000 x Nt: samples\n",
    "            H, H_oG, Gamma, Steering = Generate_channel(Nt, Nr, d_H, array_type, elements_nor, N_type, Nr_ULA, Nr_UCA)\n",
    "                \n",
    "            HH = np.concatenate((np.concatenate((H_oG.real, H_oG.imag), axis=1),\n",
    "                                np.concatenate((-H_oG.imag, H_oG.real), axis=1)), axis=0)\n",
    "            \n",
    "            x = np.zeros((2*Nt, NumSubcarriers))\n",
    "            a = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
    "            DataRaw = np.zeros((Nt, a))\n",
    "            for t in range(Nt):\n",
    "                #\"data symbol generate\"\n",
    "                NumBits = calculate_bits(Modulation, NumSubcarriers, NumDataSymb)\n",
    "                bit = np.random.randint(1, 3, NumBits)-1\n",
    "                DataRaw[t, :] = bit\n",
    "                for j in range(4):\n",
    "                    DataSet_b.append(bit[j])\n",
    "                I = np.zeros((1, a))\n",
    "                I[0, :] = DataRaw[t, :]\n",
    "                (dataI, dataQ) = slicer(I[0])\n",
    "\n",
    "                # Mapper\n",
    "                mapI = mapper_16QAM(QAM16, dataI)\n",
    "                mapQ = mapper_16QAM(QAM16, dataQ)\n",
    "                x[t] = mapI[0]\n",
    "                x[t+Nt] = mapQ[0]\n",
    "\n",
    "            # transpose\n",
    "            x = x.transpose()\n",
    "\n",
    "            y_wo_noise = np.matmul(x, HH)\n",
    "\n",
    "            # noise\n",
    "            noise = AWGN(y_wo_noise.transpose(), snr)\n",
    "\n",
    "            y = y_wo_noise + noise.transpose()\n",
    "\n",
    "            DataSet_x.append(x)    # ! I, Q sample distance by Nt.\n",
    "            DataSet_y.append(y)                 # ! output sample\n",
    "            \n",
    "            # Imperfect channel: 5%\n",
    "            DataSet_Steering.append(np.squeeze(Steering))\n",
    "            DataSet_HH.append(HH)\n",
    "            DataSet_H.append(H)               # ! Generated channel\n",
    "\n",
    "\n",
    "    # Shuffle dataset\n",
    "    random.seed(1)\n",
    "    temp = list(zip(DataSet_x, DataSet_y, DataSet_H, DataSet_HH, DataSet_Steering))\n",
    "    random.shuffle(temp)\n",
    "    DataSet_x, DataSet_y, DataSet_H, DataSet_HH, DataSet_Steering = zip(*temp)\n",
    "\n",
    "    return DataSet_x, DataSet_y, DataSet_H, DataSet_HH, DataSet_Steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_channel (H):\n",
    "# H_raw = [R(H) I(H); -I(H) R(H)]\n",
    "# we have four version of H_est\n",
    "    H_est_1 = []\n",
    "    H_est_2 = []\n",
    "    H_est_3 = []\n",
    "    H_est_4 = []\n",
    "\n",
    "    H_est_Re_1 = H[0:Nt, 0:Nr]\n",
    "    H_est_Im_1 = H[0:Nt, Nr:2*Nr]\n",
    "    H_est_Im_2 = - H[Nt:2*Nt, 0:Nr]\n",
    "    H_est_Re_2 = H[Nt:2*Nt, Nr:2*Nr]\n",
    "\n",
    "    H_est_1 = H_est_Re_1 + 1j * H_est_Im_1\n",
    "    H_est_2 = H_est_Re_1 + 1j * H_est_Im_2\n",
    "    H_est_3 = H_est_Re_2 + 1j * H_est_Im_1\n",
    "    H_est_4 = H_est_Re_2 + 1j * H_est_Im_2\n",
    "    \n",
    "    return H_est_1, H_est_2, H_est_3, H_est_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMSE(H_est, H_raw):\n",
    "    H_est_1, H_est_2, H_est_3, H_est_4 = reconstruct_channel(H_est)\n",
    "    H_est_vec_1 = torch.reshape(H_est_1, [Nt * Nr, 1])\n",
    "    H_est_vec_2 = torch.reshape(H_est_2, [Nt * Nr, 1])\n",
    "    H_est_vec_3 = torch.reshape(H_est_3, [Nt * Nr, 1])\n",
    "    H_est_vec_4 = torch.reshape(H_est_4, [Nt * Nr, 1])\n",
    "\n",
    "    H_raw_vec = torch.reshape(H_raw, [Nt * Nr, 1])\n",
    "\n",
    "    mse_1       = (torch.norm(H_raw_vec - H_est_vec_1)**2) / len(H_raw_vec)\n",
    "    mse_2       = (torch.norm(H_raw_vec - H_est_vec_2)**2) / len(H_raw_vec)\n",
    "    mse_3       = (torch.norm(H_raw_vec - H_est_vec_3)**2) / len(H_raw_vec)\n",
    "    mse_4       = (torch.norm(H_raw_vec - H_est_vec_4)**2) / len(H_raw_vec)\n",
    "\n",
    "    sigEner   = torch.norm(H_raw_vec)**2\n",
    "\n",
    "    nmse_1      = mse_1 / sigEner\n",
    "    nmse_2      = mse_2 / sigEner\n",
    "    nmse_3      = mse_3 / sigEner\n",
    "    nmse_4      = mse_4 / sigEner\n",
    "\n",
    "    # Best nmse\n",
    "    nmse        = min([nmse_1, nmse_2, nmse_3, nmse_4])\n",
    "\n",
    "    # E = H_raw - H_est\n",
    "    \n",
    "    # # Tính tổng các bình phương của sự khác biệt\n",
    "    # sum_squares_E = torch.sum(E * torch.conj(E))\n",
    "    \n",
    "    # # Tính tổng các bình phương của các phần tử của ma trận H\n",
    "    # sum_squares_H = torch.sum(H_raw * torch.conj(H_raw))\n",
    "    \n",
    "    # # Tính NMSE\n",
    "    # nmse = sum_squares_E / sum_squares_H\n",
    "\n",
    "    return torch.abs(nmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "pyjIgxUurU0P"
   },
   "outputs": [],
   "source": [
    "def Input_ISDNN(mode, DataSet_x, DataSet_y, DataSet_H, DataSet_HH, DataSet_Steering, N_samp):\n",
    "    H_in = []        # ! H_in    , np.diag(np.diag()) return a diag matrix instead of diag components.\n",
    "    H_true = []   # ! generated s\n",
    "    H_raw = []\n",
    "    # y = []\n",
    "    e = []        # ! vector errors\n",
    "    xTx = []\n",
    "    xTy = []\n",
    "    Di = []\n",
    "    steering = [] # ! Steering vector: ZoA and AoA\n",
    "\n",
    "    if mode == 'train':\n",
    "        n_sample = N_samp * len(SNR)\n",
    "    else:\n",
    "        n_sample = N_samp\n",
    "        \n",
    "    for i in range (n_sample):\n",
    "        H_true.append(torch.tensor(DataSet_HH[i]))\n",
    "        H_raw.append(torch.tensor(DataSet_H[i]))\n",
    "        # y.append(torch.tensor(DataSet_y[i]))\n",
    "        Di.append(torch.tensor(np.linalg.pinv(np.diag(np.diag(np.dot(DataSet_x[i].transpose(), DataSet_x[i]))))))\n",
    "        xTy.append(torch.tensor(np.dot(DataSet_x[i].transpose(), DataSet_y[i])))\n",
    "        H_in.append(torch.matmul(Di[i], xTy[i]))\n",
    "        e.append(torch.rand([2*Nt, 2*Nr]))\n",
    "        xTx.append(torch.tensor(np.dot(DataSet_x[i].transpose(), DataSet_x[i])))\n",
    "        steering.append(torch.tensor(DataSet_Steering[i]))\n",
    "\n",
    "    H_true = torch.stack(H_true, dim=0)\n",
    "    H_raw = torch.stack(H_raw, dim=0)\n",
    "    H_in = torch.stack(H_in, dim=0)\n",
    "    # y = torch.stack(y, dim=0)\n",
    "    e = torch.stack(e, dim=0)\n",
    "    xTx = torch.stack(xTx, dim=0)\n",
    "    xTy = torch.stack(xTy, dim=0)\n",
    "    Di = torch.stack(Di, dim=0)\n",
    "    steering = torch.stack(steering, dim=0)\n",
    "\n",
    "    return H_true, H_raw, H_in, e, xTx, xTy, Di, steering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGhdBsghq3M9"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "GHceh5kuq3ZD"
   },
   "outputs": [],
   "source": [
    "class xv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(xv, self).__init__()\n",
    "\n",
    "        self.alpha1 = torch.nn.parameter.Parameter(torch.rand(1))\n",
    "        self.alpha2 = torch.nn.parameter.Parameter(torch.tensor([0.5]))\n",
    "\n",
    "    def forward(self, Di, H, e, xTx, xTy):\n",
    "\n",
    "        xTxH = torch.bmm(xTx, H)\n",
    "\n",
    "        z    = H + torch.bmm(Di, torch.sub(xTy, xTxH)) + self.alpha1 * e\n",
    "\n",
    "        e    = torch.sub(xTy, xTxH)\n",
    "\n",
    "        H    = torch.add((1 - self.alpha2) * z, self.alpha2 * H)\n",
    "\n",
    "        return H, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "woRjD7lJssRq"
   },
   "outputs": [],
   "source": [
    "class model_driven(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model_driven, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(2*Nr, 2*Nr)\n",
    "        self.fc2 = torch.nn.Linear(2*Nr, 2*Nr)\n",
    "        self.fc3 = torch.nn.Linear(2*Nr, 2*Nr)\n",
    "        self.fc4 = torch.nn.Linear(2*Nr, 2*Nr)\n",
    "        self.fc5 = torch.nn.Linear(2*Nr, 2*Nr)\n",
    "        self.fc6 = torch.nn.Linear(2*Nr, 2*Nr)\n",
    "        self.fc7 = torch.nn.Linear(2*Nr, 2*Nr)\n",
    "        self.fc8 = torch.nn.Linear(2*Nr, 2*Nr)\n",
    "\n",
    "        self.layer1=xv()\n",
    "        self.layer2=xv()\n",
    "        self.layer3=xv()\n",
    "        self.layer4=xv()\n",
    "    \n",
    "    def forward(self, Di, H_in, e, xTx, xTy):\n",
    "        e = self.fc1(e)\n",
    "        e = self.fc2(e)\n",
    "\n",
    "        H, e = self.layer1(Di, H_in, e, xTx, xTy)\n",
    "        H = torch.tanh(H)\n",
    "\n",
    "        e = self.fc3(e)\n",
    "        e = self.fc4(e)\n",
    "        H, e = self.layer2(Di, H, e, xTx, xTy)\n",
    "        H = torch.tanh(H)\n",
    "\n",
    "        e = self.fc5(e)\n",
    "        e = self.fc6(e)\n",
    "        H, e = self.layer3(Di, H, e, xTx, xTy)\n",
    "        H = torch.tanh(H)\n",
    "\n",
    "        e = self.fc7(e)\n",
    "        e = self.fc8(e)\n",
    "        H, e = self.layer4(Di, H, e, xTx, xTy)\n",
    "\n",
    "        return H, e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ZjihTOXq3kG"
   },
   "source": [
    "# Define model, optimizer, and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Vp9fRd3gq3tw"
   },
   "outputs": [],
   "source": [
    "def def_model():\n",
    "    model = model_driven()\n",
    "    loss = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    folder_model = './model/'\n",
    "    \n",
    "    if not os.path.isdir(folder_model):\n",
    "        os.makedirs(folder_model)\n",
    "    \n",
    "    file_model = folder_model + 'H'\n",
    "    # if os.path.isfile(file_model):\n",
    "    #     generator = torch.load(file_model)\n",
    "\n",
    "    record_file = 'H'\n",
    "    return model, loss, optimizer, record_file, file_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYWM7SzItKzS"
   },
   "source": [
    "# Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "jv7lDwyxtFe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training...\n",
      "151.81494249557934\n",
      "151.4439585628374\n",
      "150.056862397101\n",
      "148.96965160041924\n",
      "151.83364380110564\n",
      "152.5429018536354\n",
      "149.99335773560293\n",
      "146.52119928216143\n",
      "1 0.48151527105219205 145.05118922087237\n",
      "100 0.13401732386088797 54.01960500736977\n",
      "200 0.07087719327048214 31.425596568928512\n",
      "300 0.0436479373615533 20.43954460342742\n",
      "400 0.028148564679492526 13.61598562278033\n",
      "500 0.018193741072513713 8.972124107175722\n",
      "600 0.011312953086065246 5.576794751247635\n",
      "700 0.006854386925176191 3.11734213086462\n",
      "800 0.0042908662674924734 1.5890535130930283\n",
      "900 0.003292058426438394 1.0188905800008263\n",
      "1000 0.0029584471125842183 0.8453461353710782\n",
      "1100 0.002804284574460724 0.7707897501598543\n",
      "1200 0.0027065621899770216 0.7241375170070676\n",
      "1300 0.0026268518866994386 0.6870143404543024\n",
      "1400 0.002554244912705554 0.6553051978419109\n",
      "1500 0.0024882985080325664 0.6274205531755949\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m         H_f[\u001b[38;5;241m0\u001b[39m \u001b[38;5;241m+\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m bs:batch_size \u001b[38;5;241m*\u001b[39m (bs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), :, :] \u001b[38;5;241m=\u001b[39m H_o\n\u001b[0;32m     39\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m loss(H_o, \n\u001b[0;32m     40\u001b[0m                           H_true[\u001b[38;5;241m0\u001b[39m \u001b[38;5;241m+\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m bs:batch_size \u001b[38;5;241m*\u001b[39m (bs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m), :, :])   \u001b[38;5;66;03m# calculate loss for the predicted output  \u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m         \u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# backpropagate the loss \u001b[39;00m\n\u001b[0;32m     42\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()        \u001b[38;5;66;03m# adjust parameters based on the calculated gradients \u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\SON\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SON\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch         = 0\n",
    "expected_epoch = 20000\n",
    "num_samp      = N_samp * len(SNR)\n",
    "best_nmse     = 1e9\n",
    "early_stop    = 0\n",
    "best_model    = ''\n",
    "batch_size    = int(num_samp / 8)\n",
    "DataSet_x, DataSet_y, DataSet_H, DataSet_HH, DataSet_Steering = Gen_dataset('train', 'UCyA', 0, 0, N_samp)\n",
    "H_true, H_raw, H_in, e, xTx, xTy, Di, steering = Input_ISDNN('train', DataSet_x, DataSet_y, DataSet_H, DataSet_HH, DataSet_Steering, N_samp)\n",
    "\n",
    "print(\"Begin training...\") \n",
    "\n",
    "while(True):\n",
    "        epoch = epoch + 1 \n",
    "\n",
    "        init_loss = 1e9\n",
    "        while( epoch == 1 and init_loss > 150):\n",
    "                \n",
    "                model, loss, optimizer, record_file, file_model = def_model()\n",
    "                for bs in range (int(num_samp / batch_size)):\n",
    "                    H_1, e_1 = model(Di[0 + batch_size * bs:batch_size * (bs+1), :, :], \n",
    "                                 H_in[0 + batch_size * bs:batch_size * (bs+1), :, :], \n",
    "                                 e[0 + batch_size * bs:batch_size * (bs+1), :, :], \n",
    "                                 xTx[0 + batch_size * bs:batch_size * (bs+1), :, :], \n",
    "                                 xTy[0 + batch_size * bs:batch_size * (bs+1), :, :])   # predict output from the model\n",
    "                    init_loss = loss(H_1, H_true[0 + batch_size * bs:batch_size * (bs+1), :, :]).item()\n",
    "                    print(init_loss)\n",
    "\n",
    "        optimizer.zero_grad()   # zero the parameter gradients\n",
    "        train_loss = 0\n",
    "        H_f = torch.empty([num_samp, 2*Nt, 2*Nr])\n",
    "        for bs in range (int(num_samp / batch_size)):\n",
    "                H_o, e_o = model(Di[0 + batch_size * bs:batch_size * (bs+1), :, :], \n",
    "                                 H_in[0 + batch_size * bs:batch_size * (bs+1), :, :], \n",
    "                                 e[0 + batch_size * bs:batch_size * (bs+1), :, :], \n",
    "                                 xTx[0 + batch_size * bs:batch_size * (bs+1), :, :], \n",
    "                                 xTy[0 + batch_size * bs:batch_size * (bs+1), :, :])   # predict output from the model\n",
    "                H_f[0 + batch_size * bs:batch_size * (bs+1), :, :] = H_o\n",
    "                train_loss = loss(H_o, \n",
    "                                  H_true[0 + batch_size * bs:batch_size * (bs+1), :, :])   # calculate loss for the predicted output  \n",
    "                train_loss.backward()   # backpropagate the loss \n",
    "                optimizer.step()        # adjust parameters based on the calculated gradients \n",
    "\n",
    "        if (epoch % 100 == 0 or epoch == 1):\n",
    "                nmse = 0\n",
    "                for j in range (num_samp):\n",
    "                        nmse += NMSE(H_f[j], H_raw[j])\n",
    "                nmse = nmse / num_samp\n",
    "                \n",
    "                if (nmse <= best_nmse):\n",
    "                        torch.save(model.state_dict(), file_model + '_' + str(epoch) + '.pth')\n",
    "                        best_model = file_model + '_' + str(epoch) + '.pth'\n",
    "                        best_nmse = nmse\n",
    "                        early_stop = 0\n",
    "                else:\n",
    "                        early_stop += 1\n",
    "\n",
    "                if (nmse > best_nmse and early_stop == 3):\n",
    "                        with Record(record_file + '_log.txt'):\n",
    "                                print(epoch, nmse.item(), train_loss.item()) \n",
    "                        break\n",
    "\n",
    "                with Record(record_file + '_log.txt'):\n",
    "                        print(epoch, nmse.item(), train_loss.item()) \n",
    "\n",
    "        if epoch  == expected_epoch:\n",
    "                torch.save(model.state_dict(), file_model + '_' + str(epoch) + '.pth')\n",
    "                best_model = file_model + '_' + str(epoch) + '.pth'\n",
    "                with Record(record_file + '_log.txt'):\n",
    "                        print(\"epoch:\\n\", epoch)\n",
    "                        print(\"Latest NMSE:\\n\", nmse.item())\n",
    "                        print(\"Latest Loss:\\n\", train_loss.item()) \n",
    "\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOBK-_l-tRMO"
   },
   "source": [
    "# Test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRRFGAX4tg_6"
   },
   "source": [
    "# Function to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfvpexfwthNq"
   },
   "outputs": [],
   "source": [
    "# from scipy.io import savemat\n",
    "\n",
    "def test(H_raw, Di, H_in, e, xTx, xTy, N_test, log): \n",
    "    # Load the model that we saved at the end of the training loop \n",
    "    model = model_driven()\n",
    "    model.load_state_dict(torch.load(best_model)) \n",
    "      \n",
    "    with torch.no_grad(): \n",
    "        H_o, e_o = model(Di, H_in, e, xTx, xTy)\n",
    "\n",
    "        nmse = 0\n",
    "        for j in range (N_test):\n",
    "                # tmp =  H_o[j]\n",
    "                # tmp1 = tmp.numpy()\n",
    "                # savemat('H_est.mat', {'H_o': tmp1})\n",
    "                nmse += NMSE(H_o[j], H_raw[j])\n",
    "\n",
    "        nmse = nmse / N_test\n",
    "        with Record(log):\n",
    "            print(format(nmse.item(), '.7f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate dataset for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "SNR_min_dB  = 0\n",
    "SNR_max_dB  = 20\n",
    "step_dB     = 2\n",
    "num_dB      = int((SNR_max_dB - SNR_min_dB) / step_dB) + 1\n",
    "\n",
    "SNR         = np.linspace(SNR_min_dB, SNR_max_dB, num=num_dB)\n",
    "log         = './model/log_test.txt'\n",
    "\n",
    "N_test = int(N_samp / 10)\n",
    "\n",
    "for i in range (100):\n",
    "    for snr in SNR:\n",
    "        with Record(log):\n",
    "            print(snr)\n",
    "        DataSet_x, DataSet_y, DataSet_H, DataSet_HH, DataSet_Steering = Gen_dataset('test', 'UCyA', 0, 0, N_test)\n",
    "        H_true, H_raw, H_in, e, xTx, xTy, Di, steering = Input_ISDNN('test', DataSet_x, DataSet_y, DataSet_H, DataSet_HH, DataSet_Steering, N_test)\n",
    "\n",
    "        \n",
    "        test(H_raw, Di, H_in, e, xTx, xTy, N_test, log)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "b540dc7c34b7cf08e01b749cd784ab7b1f302abefb3330aee8479ab39c2f7339"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
